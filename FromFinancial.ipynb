{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "B3WEPx1JJqlG"
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Library\n",
    "# ====================================================\n",
    "import sys\n",
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import random\n",
    "import re\n",
    "import unicodedata\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import pickle\n",
    "import itertools\n",
    "from tqdm.auto import tqdm\n",
    "import category_encoders as ce\n",
    "import torch\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, train_test_split, GroupKFold\n",
    "from sklearn.metrics import log_loss, roc_auc_score, matthews_corrcoef, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import Pool, CatBoostRegressor, CatBoostClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Activation\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import Callback\n",
    "from keras.models import clone_model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "pd.set_option('display.max_columns',1000)\n",
    "pd.set_option('display.max_rows',100)\n",
    "\n",
    "# ====================================================\n",
    "# Configurations\n",
    "# ====================================================\n",
    "class CFG:\n",
    "    VER = 100\n",
    "    AUTHOR = 'Yuta.K'\n",
    "    COMPETITION = 'SignateCup2024Summer'\n",
    "    DATA_PATH = Path('dataset')\n",
    "    OOF_DATA_PATH = Path('oof')\n",
    "    MODEL_DATA_PATH = Path('models')\n",
    "    SUB_DATA_PATH = Path('submission')\n",
    "    METHOD_LIST = ['lightgbm', 'xgboost', 'catboost']\n",
    "#     METHOD_LIST = [ 'adaboost','lightgbm', 'xgboost', 'catboost']\n",
    "    seed = 42\n",
    "    n_folds = 2\n",
    "    target_col = 'ProdTaken'\n",
    "    metric = 'auc'\n",
    "    metric_maximize_flag = True\n",
    "    num_boost_round = 500\n",
    "    early_stopping_round = 200\n",
    "    verbose = 25\n",
    "    classification_lgb_params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'auc',\n",
    "        'learning_rate': 0.05,\n",
    "        'seed': seed,\n",
    "    }\n",
    "    classification_xgb_params = {\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'auc',\n",
    "        'learning_rate': 0.05,\n",
    "        'random_state': seed,\n",
    "    }\n",
    "    classification_cat_params = {\n",
    "        'learning_rate': 0.05,\n",
    "        'iterations': num_boost_round,\n",
    "        'random_seed': seed,\n",
    "    }\n",
    "    classification_adaboost_params = {\n",
    "        'n_estimators': 100,\n",
    "        'learning_rate': 1.0,\n",
    "        'random_state': 42,\n",
    "    }\n",
    "    \n",
    "# ====================================================\n",
    "# Seed everything\n",
    "# ====================================================\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "seed_everything(CFG.seed)\n",
    "\n",
    "\n",
    "# ====================================================\n",
    "# Metric\n",
    "# ====================================================\n",
    "# AUC\n",
    "\n",
    "# ====================================================\n",
    "# LightGBM Metric\n",
    "# ====================================================\n",
    "def lgb_metric(y_pred, y_true):\n",
    "    y_true = y_true.get_label()\n",
    "    return 'auc', roc_auc_score(y_true, y_pred), CFG.metric_maximize_flag\n",
    "\n",
    "# ====================================================\n",
    "# XGBoost Metric\n",
    "# ====================================================\n",
    "def xgb_metric(y_pred, y_true):\n",
    "    y_true = y_true.get_label()\n",
    "    return 'auc', roc_auc_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "g6R4KoxhL91E"
   },
   "outputs": [],
   "source": [
    "#データの読み込み\n",
    "train_df = pd.read_csv(f'{CFG.DATA_PATH}/train.csv', index_col=0)\n",
    "test_df = pd.read_csv(f'{CFG.DATA_PATH}/test.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "El2B8eayMmyZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing finished\n"
     ]
    }
   ],
   "source": [
    "def preprocessing(train_df, test_df):\n",
    "    # Age numeric\n",
    "    # 漢数字とアラビア数字のマッピング\n",
    "    kanji_to_num = {'一': 1, '二': 2, '三': 3, '四': 4, '五': 5, '六': 6, '七': 7, '八': 8, '九': 9,'十': 10, '百': 100, '千': 1000, '万': 10000,'零': 0, '〇': 0}\n",
    "    def kanji_to_arabic(kanji):\n",
    "        result = 0\n",
    "        temp = 0\n",
    "        for char in kanji:\n",
    "            value = kanji_to_num.get(char, None)\n",
    "            if value is not None:\n",
    "                if value < 10:\n",
    "                    if temp == 0:\n",
    "                        temp = value\n",
    "                    else:\n",
    "                        temp = temp * 10 + value\n",
    "                elif value >= 10:\n",
    "                    if temp == 0:\n",
    "                        temp = 1\n",
    "                    result += temp * value\n",
    "                    temp = 0\n",
    "        return result + temp\n",
    "    def process_age(age):\n",
    "        if age is None or str(age) == 'nan':\n",
    "            return None\n",
    "        age = unicodedata.normalize('NFKC', age)\n",
    "        age = ''.join([c for c in age if c.isdigit() or c in kanji_to_num])\n",
    "        if age.isdigit():\n",
    "            return int(age)\n",
    "        return kanji_to_arabic(age)\n",
    "\n",
    "    # TypeofContact categorical(dummy)\n",
    "    def TypeofContact_to_dummy(str):\n",
    "        if str == 'Self Enquiry':\n",
    "            return 1\n",
    "        elif str == 'Company Invited':\n",
    "            return 0\n",
    "        \n",
    "    # CityTier 順序尺度\n",
    "    \n",
    "    # DurationOfPitch numeric\n",
    "    def convert_to_minutes(duration):\n",
    "        # durationがfloat型またはNoneである可能性があるため、文字列であることを確認\n",
    "        if pd.isnull(duration):\n",
    "            return None  # NaNの場合、Noneを返す\n",
    "        duration = str(duration)  # 文字列に変換してエラーを防ぐ\n",
    "        if '分' in duration:\n",
    "            return float(duration.replace('分', ''))\n",
    "        elif '秒' in duration:\n",
    "            return float(duration.replace('秒', '')) / 60  # 秒を分に変換し、整数で返す\n",
    "\n",
    "    # Occupation categorical\n",
    "    def Occupation_to_dummy(str):\n",
    "        if str == 'Large Business':\n",
    "            return 2\n",
    "        elif str == 'Small Business':\n",
    "            return 1\n",
    "        elif str == 'Salaried':\n",
    "            return 0\n",
    "       \n",
    "    # Gender categorical\n",
    "    def Gender_dealing(gender):\n",
    "        # 文字列を半角に変換し、大文字に統一\n",
    "        gender = unicodedata.normalize('NFKC', gender).upper().strip()\n",
    "        # 不要な空白を削除\n",
    "        gender = ''.join(gender.split())\n",
    "\n",
    "        if 'FEMALE' in gender:\n",
    "            return 1\n",
    "        elif 'MALE' in gender:\n",
    "            return 0\n",
    "        else:\n",
    "            return None  # 性別が識別できない場合はNoneを返す\n",
    "        \n",
    "    # NumberOfPersonVisiting numeric\n",
    "    \n",
    "    # NumberOfFollowups numeric\n",
    "    def NumberOfFollowups_dealing(input_int):\n",
    "        if input_int >= 100:\n",
    "            return input_int /100\n",
    "        else:\n",
    "            return input_int\n",
    "    \n",
    "    # ProductPitched categorical\n",
    "    # Designation categorical\n",
    "    def standardize_str(input_str):\n",
    "        # 文字列を半角に変換し、小文字に統一\n",
    "        input_str = unicodedata.normalize('NFKC', input_str).lower().strip()\n",
    "        # 不要な空白や特殊記号を削除\n",
    "        input_str = ''.join(input_str.split())\n",
    "        input_str = input_str.replace('|', 'l').replace('×', 'x').replace('𝘤', 'c').replace('𝖺', 'a').replace('𝙳', 'd')\n",
    "        # その他特殊文字を通常の英字に置換\n",
    "        input_str = input_str.replace('ᗞ', 'd').replace('𐊡', 'a').replace('𝘳', 'r').replace('ꓢ', 's').replace('ı', 'i')\n",
    "        input_str = input_str.replace('β', 'b').replace('в', 'b').replace('с', 'c').replace('տ', 's').replace('ς', 'c')\n",
    "        input_str = input_str.replace('ꭰ', 'd').replace('ε', 'e').replace('ι', 'i').replace('α', 'a').replace('ո', 'n')\n",
    "        input_str = input_str.replace('ѕ', 's').replace('μ', 'm').replace('е', 'e').replace('а', 'a').replace('ѵ', 'v')\n",
    "        input_str = input_str.replace('aasic', 'basic')\n",
    "        return input_str\n",
    "    \n",
    "    # PreferredPropertyStar 順序尺度\n",
    "    \n",
    "    # NumberOfTrips numeric\n",
    "    def NumberOfTrips_dealing(str):\n",
    "        if pd.isnull(str):\n",
    "            return None \n",
    "        if '半年に' in str:\n",
    "            return 2 * int(str.replace('半年に', '').replace('回', ''))\n",
    "        elif '年に' in str:\n",
    "            return int(str.replace('年に', '').replace('回', ''))\n",
    "        elif '四半期に' in str:\n",
    "            return 4 * int(str.replace('四半期に', '').replace('回', ''))\n",
    "        else :\n",
    "            return int(str)\n",
    "        \n",
    "    # Passport categorical(dummy)\n",
    "    \n",
    "    # PitchSatisfactionScore 順序尺度だけど間隔尺度的要素あり\n",
    "    \n",
    "    # MonthlyIncome numeric\n",
    "    def MonthlyIncome_dealing(input_str):\n",
    "        if pd.isnull(input_str):\n",
    "            return None \n",
    "        if '月収' in input_str:\n",
    "            return 10000 * float(input_str.replace('月収', '').replace('万円', ''))\n",
    "        elif '万円' in input_str:\n",
    "            return 10000 * float(input_str.replace('万円', ''))\n",
    "        else:\n",
    "            return float(input_str)\n",
    "        \n",
    "    # customer_info\n",
    "    def customer_info_dealing(input_str):\n",
    "        # 文字列を半角に変換し、小文字に統一\n",
    "        input_str = unicodedata.normalize('NFKC', input_str).lower().strip()\n",
    "        # 不要な空白や特殊記号を削除\n",
    "        input_str = input_str.replace('/', ' ').replace('／', ' ').replace('、', ' ').replace('　', ' ')\n",
    "        input_str = input_str.replace('\\u3000', ' ').replace('\\t', ' ').replace('\\n', ' ')\n",
    "        input_str = re.sub(r'(?<=\\S)\\s+(?=\\S)', ',', input_str, count=2)\n",
    "        return input_str\n",
    "    \n",
    "    # married categorical\n",
    "    \n",
    "    # car_possesion categorival(dummy)\n",
    "    def car_possesion_dealing(input_str):\n",
    "        if input_str in ['車未所持', '自動車未所有', '自家用車なし', '乗用車なし', '車なし', '車保有なし', 0]:\n",
    "            return 0\n",
    "        elif input_str in ['車所持', '自動車所有', '自家用車あり', '乗用車所持', '車保有', '車あり', 1]:\n",
    "            return 1\n",
    "        \n",
    "    # offspring -1以外はnumeric\n",
    "    def offspring_dealing(input_str):\n",
    "        if '1' in input_str:\n",
    "            return 1\n",
    "        elif '2' in input_str:\n",
    "            return 2\n",
    "        elif '3' in input_str:\n",
    "            return 3\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    def offspring_identified_dealing(input_str):\n",
    "        if input_str in ['子供の数不明', '不明', 'わからない', '子育て状況不明', '子の数不詳']:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    def dealing_missing_values(input_df):\n",
    "        df = input_df.copy()\n",
    "        df['Age'] = df['Age'].fillna(df['Age'].mean())\n",
    "        df['TypeofContact'] = df['TypeofContact'].fillna(df['TypeofContact'].median())\n",
    "        df['DurationOfPitch'] = df['DurationOfPitch'].fillna(df['DurationOfPitch'].mean())\n",
    "        df['NumberOfFollowups'] = df['NumberOfFollowups'].fillna(df['NumberOfFollowups'].mean())\n",
    "        df['NumberOfTrips'] = df['NumberOfTrips'].fillna(df['NumberOfTrips'].mean())\n",
    "        df['MonthlyIncome'] = df['MonthlyIncome'].fillna(df['MonthlyIncome'].mean())\n",
    "        return df\n",
    "    \n",
    "    def dummy_ex(feature, train_df, test_df):\n",
    "        # OneHotEncoder の初期化時に sparse_output 引数を使用\n",
    "        ohe = OneHotEncoder(sparse_output=False, categories='auto')\n",
    "        new_array = pd.concat([train_df[[feature]], test_df[[feature]]], axis=0)\n",
    "        ohe.fit(new_array)\n",
    "\n",
    "        # ダミー変数の列名の作成\n",
    "        columns = [f'{feature}_{v}' for v in ohe.categories_[0]]\n",
    "\n",
    "        # 生成されたダミー変数をデータフレームに変換\n",
    "        dummy_vals_train = pd.DataFrame(ohe.transform(train_df[[feature]]), columns=columns)\n",
    "        dummy_vals_test = pd.DataFrame(ohe.transform(test_df[[feature]]), columns=columns)\n",
    "\n",
    "        # 残りの変数と結合\n",
    "        tr = pd.concat([train_df.drop([feature], axis=1), dummy_vals_train.reset_index(drop=True)], axis=1)\n",
    "        te = pd.concat([test_df.drop([feature], axis=1), dummy_vals_test.reset_index(drop=True)], axis=1)\n",
    "\n",
    "        return tr, te\n",
    "    \n",
    "    def function_apply(input_df):\n",
    "        df = input_df.copy()\n",
    "        df['Age'] = df['Age'].apply(process_age)\n",
    "        df['TypeofContact'] = df['TypeofContact'].apply(TypeofContact_to_dummy)\n",
    "        df['DurationOfPitch'] = df['DurationOfPitch'].apply(convert_to_minutes)\n",
    "        df['Occupation'] = df['Occupation'].apply(Occupation_to_dummy)\n",
    "        df['Gender'] = df['Gender'].apply(Gender_dealing)\n",
    "        df['NumberOfFollowups'] = df['NumberOfFollowups'].apply(NumberOfFollowups_dealing)\n",
    "        df['ProductPitched'] = df['ProductPitched'].apply(standardize_str)\n",
    "        df['NumberOfTrips'] = df['NumberOfTrips'].apply(NumberOfTrips_dealing)\n",
    "        df['Designation'] = df['Designation'].apply(standardize_str)\n",
    "        df['MonthlyIncome'] = df['MonthlyIncome'].apply(MonthlyIncome_dealing)\n",
    "        df['customer_info'] = df['customer_info'].apply(customer_info_dealing)\n",
    "        df[['married', 'car_possesion', 'offspring']] = df['customer_info'].str.split(',', n=2, expand=True)\n",
    "        df = df.drop(['customer_info'],axis=1)\n",
    "        df['car_possesion'] = df['car_possesion'].apply(car_possesion_dealing)\n",
    "        df['offspring'] = df['offspring'].apply(offspring_dealing)\n",
    "        df['offspring_identified'] = df['offspring'].apply(offspring_identified_dealing)\n",
    "        df = dealing_missing_values(df)\n",
    "        return df\n",
    "    \n",
    "    dummy_col = ['CityTier', 'Occupation', 'ProductPitched', 'PreferredPropertyStar', 'PitchSatisfactionScore', 'Designation', 'married']\n",
    "    std_feature = ['Age', 'DurationOfPitch', 'NumberOfPersonVisiting', 'NumberOfFollowups', 'NumberOfTrips', 'MonthlyIncome']\n",
    "    \n",
    "    def function_apply_both(train_df, test_df, std_feature):\n",
    "        tr_df = train_df.copy()\n",
    "        te_df = test_df.copy()\n",
    "        tr_df = function_apply(tr_df)\n",
    "        te_df = function_apply(te_df)\n",
    "        for feature in dummy_col:\n",
    "            tr_df, te_x = dummy_ex(feature, tr_df, te_df)\n",
    "        std_sc = StandardScaler()\n",
    "        tr_df[std_feature] = std_sc.fit_transform(tr_df[std_feature])\n",
    "        te_df[std_feature] = std_sc.fit_transform(te_df[std_feature])\n",
    "        return tr_df, te_df\n",
    "    \n",
    "    tr_df, te_df = function_apply_both(train_df, test_df, std_feature)\n",
    "    \n",
    "    print('Preprocessing finished')\n",
    "    return tr_df, te_df\n",
    "\n",
    "train_df, test_df = preprocessing(train_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "quaQcTgQOjyJ"
   },
   "outputs": [],
   "source": [
    "#Learning & Predicting\n",
    "\n",
    "#1段階目の学習\n",
    "def Pre_Learning(train_df,test_df, features, categorical_features):\n",
    "    \n",
    "    #adaboostでの学習メソッドの定義\n",
    "    def adaboost_training(x_train, y_train, x_valid, y_valid, features, categorical_features):\n",
    "        model = AdaBoostClassifier(**CFG.classification_adaboost_params)\n",
    "        model.fit(x_train, y_train)\n",
    "        # Predict validation\n",
    "        valid_pred = model.predict_proba(x_valid)[:, 1]\n",
    "        return model, valid_pred\n",
    "\n",
    "    #lightgbmでの学習メソッドの定義\n",
    "    def lightgbm_training(x_train, y_train, x_valid, y_valid, features, categorical_features):\n",
    "        lgb_train = lgb.Dataset(x_train, y_train, categorical_feature=categorical_features)\n",
    "        lgb_valid = lgb.Dataset(x_valid, y_valid, categorical_feature=categorical_features)\n",
    "        model = lgb.train(\n",
    "                    params = CFG.classification_lgb_params,\n",
    "                    train_set = lgb_train,\n",
    "                    num_boost_round = CFG.num_boost_round,\n",
    "                    valid_sets = [lgb_train, lgb_valid],\n",
    "                    feval = lgb_metric,\n",
    "                    callbacks=[lgb.early_stopping(stopping_rounds=CFG.early_stopping_round,\n",
    "                                                  verbose=CFG.verbose)]\n",
    "                )\n",
    "        # Predict validation\n",
    "        valid_pred = model.predict(x_valid)\n",
    "        return model, valid_pred\n",
    "\n",
    "    #xgboostでの学習メソッドの定義\n",
    "    def xgboost_training(x_train, y_train, x_valid, y_valid, features, categorical_features):\n",
    "        xgb_train = xgb.DMatrix(data=x_train, label=y_train)\n",
    "        xgb_valid = xgb.DMatrix(data=x_valid, label=y_valid)\n",
    "        model = xgb.train(\n",
    "                    CFG.classification_xgb_params,\n",
    "                    dtrain = xgb_train,\n",
    "                    num_boost_round = CFG.num_boost_round,\n",
    "                    evals = [(xgb_train, 'train'), (xgb_valid, 'eval')],\n",
    "                    early_stopping_rounds = CFG.early_stopping_round,\n",
    "                    verbose_eval = CFG.verbose,\n",
    "                    feval = xgb_metric,\n",
    "                    maximize = CFG.metric_maximize_flag,\n",
    "                )\n",
    "        # Predict validation\n",
    "        valid_pred = model.predict(xgb.DMatrix(x_valid))\n",
    "        return model, valid_pred\n",
    "\n",
    "    #catboostでの学習メソッドの定義\n",
    "    def catboost_training(x_train, y_train, x_valid, y_valid, features, categorical_features):\n",
    "        cat_train = Pool(data=x_train, label=y_train, cat_features=categorical_features)\n",
    "        cat_valid = Pool(data=x_valid, label=y_valid, cat_features=categorical_features)\n",
    "        model = CatBoostClassifier(**CFG.classification_cat_params)\n",
    "        model.fit(cat_train,\n",
    "                  eval_set = [cat_valid],\n",
    "                  early_stopping_rounds = CFG.early_stopping_round,\n",
    "                  verbose = CFG.verbose,\n",
    "                  use_best_model = True)\n",
    "        # Predict validation\n",
    "        valid_pred = model.predict_proba(x_valid)[:, 1]\n",
    "        return model, valid_pred\n",
    "\n",
    "    #任意のモデルでのクロスバリデーション学習メソッドの定義\n",
    "    def gradient_boosting_model_cv_training(method, train_df, features, categorical_features):\n",
    "        # Create a numpy array to store out of folds predictions\n",
    "        oof_predictions = np.zeros(len(train_df))\n",
    "        oof_fold = np.zeros(len(train_df))\n",
    "        kfold = KFold(n_splits=CFG.n_folds, shuffle=True, random_state=CFG.seed)\n",
    "        for fold, (train_index, valid_index) in enumerate(kfold.split(train_df)):\n",
    "            print('-'*50)\n",
    "            print(f'{method} training fold {fold+1}')\n",
    "\n",
    "            x_train = train_df[features].iloc[train_index]\n",
    "            y_train = train_df[CFG.target_col].iloc[train_index]\n",
    "            x_valid = train_df[features].iloc[valid_index]\n",
    "            y_valid = train_df[CFG.target_col].iloc[valid_index]\n",
    "\n",
    "            model = None  # モデル変数を初期化する\n",
    "            valid_pred = None\n",
    "\n",
    "            if method == 'adaboost':\n",
    "                model, valid_pred = adaboost_training(x_train, y_train, x_valid, y_valid, features, categorical_features)\n",
    "            if method == 'lightgbm':\n",
    "                model, valid_pred = lightgbm_training(x_train, y_train, x_valid, y_valid, features, categorical_features)\n",
    "            if method == 'xgboost':\n",
    "                model, valid_pred = xgboost_training(x_train, y_train, x_valid, y_valid, features, categorical_features)\n",
    "            if method == 'catboost':\n",
    "                model, valid_pred = catboost_training(x_train, y_train, x_valid, y_valid, features, categorical_features)  \n",
    "            # Save best model\n",
    "            pickle.dump(model, open(f'{method}_fold{fold + 1}_seed{CFG.seed}_ver{CFG.VER}.pkl', 'wb'))\n",
    "            # Add to out of folds array\n",
    "            oof_predictions[valid_index] = valid_pred\n",
    "            oof_fold[valid_index] = fold + 1\n",
    "            del x_train, x_valid, y_train, y_valid, model, valid_pred\n",
    "            gc.collect()\n",
    "\n",
    "        # Compute out of folds metric\n",
    "        score = f1_score(train_df[CFG.target_col], oof_predictions >= 0.5, average='macro')\n",
    "        print(f'{method} our out of folds CV f1score is {score}')\n",
    "        # Create a dataframe to store out of folds predictions\n",
    "        oof_df = pd.DataFrame({CFG.target_col: train_df[CFG.target_col], f'{method}_prediction': oof_predictions, 'fold': oof_fold})\n",
    "        oof_df.to_csv(f'oof_{method}_seed{CFG.seed}_ver{CFG.VER}.csv', index = False)\n",
    "\n",
    "    #adaboostの学習済みモデル読み込み関数\n",
    "    def adaboost_inference(x_test):\n",
    "        test_pred = np.zeros(len(x_test))\n",
    "        for fold in range(CFG.n_folds):\n",
    "            model = pickle.load(open(f'adaboost_fold{fold + 1}_seed{CFG.seed}_ver{CFG.VER}.pkl', 'rb'))\n",
    "            # Predict\n",
    "            pred = model.predict_proba(x_test)[:, 1]\n",
    "            test_pred += pred\n",
    "        return test_pred / CFG.n_folds\n",
    "\n",
    "    #lightgbmの学習モデル読み込み関数\n",
    "    def lightgbm_inference(x_test):\n",
    "        test_pred = np.zeros(len(x_test))\n",
    "        for fold in range(CFG.n_folds):\n",
    "            model = pickle.load(open(f'lightgbm_fold{fold + 1}_seed{CFG.seed}_ver{CFG.VER}.pkl', 'rb'))\n",
    "            # Predict\n",
    "            pred = model.predict(x_test)\n",
    "            test_pred += pred\n",
    "        return test_pred / CFG.n_folds\n",
    "\n",
    "    #xgboostの学習モデル読み込み関数\n",
    "    def xgboost_inference(x_test):\n",
    "        test_pred = np.zeros(len(x_test))\n",
    "        for fold in range(CFG.n_folds):\n",
    "            model = pickle.load(open(f'xgboost_fold{fold + 1}_seed{CFG.seed}_ver{CFG.VER}.pkl', 'rb'))\n",
    "            # Predict\n",
    "            pred = model.predict(xgb.DMatrix(x_test))\n",
    "            test_pred += pred\n",
    "        return test_pred / CFG.n_folds\n",
    "\n",
    "    #catboostの学習モデル読み込み関数\n",
    "    def catboost_inference(x_test):\n",
    "        test_pred = np.zeros(len(x_test))\n",
    "        for fold in range(CFG.n_folds):\n",
    "            model = pickle.load(open(f'catboost_fold{fold + 1}_seed{CFG.seed}_ver{CFG.VER}.pkl', 'rb'))\n",
    "            # Predict\n",
    "            pred = model.predict_proba(x_test)[:, 1]\n",
    "            test_pred += pred\n",
    "        return test_pred / CFG.n_folds\n",
    "\n",
    "    #任意のメソッドに対して予測を返す関数\n",
    "    def gradient_boosting_model_inference(method, test_df, features, categorical_features):\n",
    "        x_test = test_df[features]\n",
    "        if method == 'adaboost':\n",
    "            test_pred = adaboost_inference(x_test)\n",
    "        if method == 'lightgbm':\n",
    "            test_pred = lightgbm_inference(x_test)\n",
    "        if method == 'xgboost':\n",
    "            test_pred = xgboost_inference(x_test)\n",
    "        if method == 'catboost':\n",
    "            test_pred = catboost_inference(x_test)\n",
    "        return test_pred\n",
    "\n",
    "    for method in CFG.METHOD_LIST:\n",
    "        gradient_boosting_model_cv_training(method, train_df, features, categorical_features)\n",
    "        test_df[f'{method}_pred_prob'] = gradient_boosting_model_inference(method, test_df, features, categorical_features)\n",
    "        \n",
    "        \n",
    "#2段階目の学習　ニューラルネットワークによるスタッキング\n",
    "def Post_Learning(train_df,test_df):\n",
    "    #ニューラルネットワークモデル作成関数\n",
    "    def create_nn_model(input_shape):\n",
    "        model = Sequential([\n",
    "            Dense(64, input_shape=(input_shape,)),\n",
    "            BatchNormalization(),\n",
    "            Activation('relu'),\n",
    "            Dropout(0.5),\n",
    "\n",
    "            Dense(32),\n",
    "            BatchNormalization(),\n",
    "            Activation('relu'),\n",
    "            Dropout(0.5),\n",
    "\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        optimizer = Adam(lr=0.001)\n",
    "        model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    #ニューラルネットワーク用学習スケジューラー\n",
    "    def scheduler(epoch, lr):\n",
    "            if epoch < 10:\n",
    "                return lr\n",
    "            else:\n",
    "                return lr * np.exp(-0.1)\n",
    "\n",
    "    #特徴量同士で積を作る関数\n",
    "    def create_interaction_features(features):\n",
    "            n_features = features.shape[1]\n",
    "            interaction_features = []\n",
    "            for i in range(n_features):\n",
    "                for j in range(i + 1, n_features):\n",
    "                    interaction_features.append(features[:, i] * features[:, j])  \n",
    "            return np.column_stack(interaction_features)\n",
    "    \n",
    "    # OOF予測を基に新たな特徴量を作成\n",
    "    oof_features = np.zeros((train_df.shape[0], len(CFG.METHOD_LIST)))\n",
    "    for i, method in enumerate(CFG.METHOD_LIST):\n",
    "        oof_df = pd.read_csv(f'oof_{method}_seed{CFG.seed}_ver{CFG.VER}.csv')\n",
    "        oof_features[:, i] = oof_df[f'{method}_prediction']\n",
    "    \n",
    "    # テストデータの予測を基に特徴量を作成\n",
    "    test_features = np.zeros((test_df.shape[0], len(CFG.METHOD_LIST)))\n",
    "    for i, method in enumerate(CFG.METHOD_LIST):\n",
    "        test_features[:, i] = test_df[f'{method}_pred_prob']\n",
    "\n",
    "    # 特徴量同士の積を追加\n",
    "    oof_interaction_features = create_interaction_features(oof_features)\n",
    "    test_interaction_features = create_interaction_features(test_features)\n",
    "\n",
    "    # 元の特徴量と相互作用特徴量を組み合わせ\n",
    "    oof_combined_features = np.hstack([oof_features, oof_interaction_features])\n",
    "    test_combined_features = np.hstack([test_features, test_interaction_features])\n",
    "\n",
    "    # 特徴量の標準化\n",
    "    global oof_combined_features_scaled, test_combined_features_scaled\n",
    "    scaler = StandardScaler()\n",
    "    oof_combined_features_scaled = scaler.fit_transform(oof_combined_features)\n",
    "    test_combined_features_scaled = scaler.transform(test_combined_features)   \n",
    "    \n",
    "    # ニューラルネットワークモデルを学習\n",
    "    nn_model = create_nn_model(oof_combined_features_scaled.shape[1])\n",
    "    callbacks_list = [LearningRateScheduler(scheduler)]\n",
    "    nn_model.fit(oof_combined_features_scaled, train_df[CFG.target_col],\n",
    "                 validation_split=0.2, epochs=50, batch_size=32, callbacks=callbacks_list, verbose=1)\n",
    "    nn_model.save(f'nn_stacking_model_seed{CFG.seed}_ver{CFG.VER}.h5')\n",
    "    \n",
    "    #ロジスティック回帰モデルを学習\n",
    "    lr_model = LogisticRegression()\n",
    "    lr_model.fit(oof_combined_features_scaled, train_df[CFG.target_col])\n",
    "    pickle.dump(lr_model, open(f'lr_stacking_model_seed{CFG.seed}_ver{CFG.VER}.pkl','wb'))\n",
    "\n",
    "def Learning_and_Predicting(train_df, test_df, features, categorical_features):\n",
    "    Pre_Learning(train_df, test_df, features, categorical_features)\n",
    "    Post_Learning(train_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kWzQv798OiQ-",
    "outputId": "57cabf2c-5c42-4084-e263-e00eaeb695ec",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "lightgbm training fold 1\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Number of positive: 18850, number of negative: 2303\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001695 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 13822\n",
      "[LightGBM] [Info] Number of data points in the train set: 21153, number of used features: 43\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.891127 -> initscore=2.102300\n",
      "[LightGBM] [Info] Start training from score 2.102300\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[24]\ttraining's auc: 0.897924\ttraining's f1score: 0.584616\tvalid_1's auc: 0.753874\tvalid_1's f1score: 0.544304\n",
      "--------------------------------------------------\n",
      "lightgbm training fold 2\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Number of positive: 18917, number of negative: 2237\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004928 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13986\n",
      "[LightGBM] [Info] Number of data points in the train set: 21154, number of used features: 43\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.894252 -> initscore=2.134925\n",
      "[LightGBM] [Info] Start training from score 2.134925\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[17]\ttraining's auc: 0.882418\ttraining's f1score: 0.494358\tvalid_1's auc: 0.766754\tvalid_1's f1score: 0.492252\n",
      "lightgbm our out of folds CV f1score is 0.5188994558145035\n",
      "--------------------------------------------------\n",
      "xgboost training fold 1\n",
      "[0]\ttrain-logloss:0.65982\ttrain-f1score:0.09818\teval-logloss:0.66014\teval-f1score:0.09563\n",
      "[25]\ttrain-logloss:0.33291\ttrain-f1score:0.67701\teval-logloss:0.34018\teval-f1score:0.65175\n",
      "[50]\ttrain-logloss:0.27178\ttrain-f1score:0.67807\teval-logloss:0.29038\teval-f1score:0.64808\n",
      "[75]\ttrain-logloss:0.24903\ttrain-f1score:0.68752\teval-logloss:0.28228\teval-f1score:0.64836\n",
      "[100]\ttrain-logloss:0.23553\ttrain-f1score:0.69878\teval-logloss:0.28106\teval-f1score:0.65009\n",
      "[125]\ttrain-logloss:0.22544\ttrain-f1score:0.71141\teval-logloss:0.28070\teval-f1score:0.65343\n",
      "[150]\ttrain-logloss:0.21787\ttrain-f1score:0.72472\teval-logloss:0.28084\teval-f1score:0.65484\n",
      "[175]\ttrain-logloss:0.21118\ttrain-f1score:0.73258\teval-logloss:0.28122\teval-f1score:0.65636\n",
      "[200]\ttrain-logloss:0.20460\ttrain-f1score:0.74316\teval-logloss:0.28169\teval-f1score:0.65741\n",
      "[212]\ttrain-logloss:0.20190\ttrain-f1score:0.74901\teval-logloss:0.28189\teval-f1score:0.65631\n",
      "--------------------------------------------------\n",
      "xgboost training fold 2\n",
      "[0]\ttrain-logloss:0.65967\ttrain-f1score:0.09563\teval-logloss:0.66016\teval-f1score:0.09818\n",
      "[25]\ttrain-logloss:0.33102\ttrain-f1score:0.65903\teval-logloss:0.34111\teval-f1score:0.65107\n",
      "[50]\ttrain-logloss:0.27069\ttrain-f1score:0.66417\teval-logloss:0.29197\teval-f1score:0.64849\n",
      "[75]\ttrain-logloss:0.24799\ttrain-f1score:0.67199\teval-logloss:0.28361\teval-f1score:0.65296\n",
      "[100]\ttrain-logloss:0.23563\ttrain-f1score:0.68586\teval-logloss:0.28200\teval-f1score:0.65558\n",
      "[125]\ttrain-logloss:0.22915\ttrain-f1score:0.69579\teval-logloss:0.28190\teval-f1score:0.65946\n",
      "[150]\ttrain-logloss:0.22199\ttrain-f1score:0.70614\teval-logloss:0.28214\teval-f1score:0.65888\n",
      "[175]\ttrain-logloss:0.21581\ttrain-f1score:0.71553\teval-logloss:0.28243\teval-f1score:0.65991\n",
      "[200]\ttrain-logloss:0.21024\ttrain-f1score:0.72735\teval-logloss:0.28290\teval-f1score:0.65961\n",
      "[208]\ttrain-logloss:0.20829\ttrain-f1score:0.73048\teval-logloss:0.28302\teval-f1score:0.65985\n",
      "xgboost our out of folds CV f1score is 0.6291361629633896\n",
      "Epoch 1/50\n",
      "1058/1058 [==============================] - 3s 2ms/step - loss: 0.3633 - accuracy: 0.8732 - val_loss: 0.2834 - val_accuracy: 0.9030 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "1058/1058 [==============================] - 2s 2ms/step - loss: 0.3037 - accuracy: 0.8969 - val_loss: 0.2818 - val_accuracy: 0.9025 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "1058/1058 [==============================] - 2s 2ms/step - loss: 0.2967 - accuracy: 0.8975 - val_loss: 0.2812 - val_accuracy: 0.9024 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "1058/1058 [==============================] - 2s 2ms/step - loss: 0.2954 - accuracy: 0.8988 - val_loss: 0.2814 - val_accuracy: 0.9023 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "1058/1058 [==============================] - 2s 2ms/step - loss: 0.2925 - accuracy: 0.8990 - val_loss: 0.2816 - val_accuracy: 0.9020 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "1058/1058 [==============================] - 2s 2ms/step - loss: 0.2907 - accuracy: 0.8988 - val_loss: 0.2820 - val_accuracy: 0.9024 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "1058/1058 [==============================] - 2s 2ms/step - loss: 0.2904 - accuracy: 0.8998 - val_loss: 0.2816 - val_accuracy: 0.9025 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "1058/1058 [==============================] - 2s 2ms/step - loss: 0.2906 - accuracy: 0.8991 - val_loss: 0.2823 - val_accuracy: 0.9026 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "1058/1058 [==============================] - 2s 2ms/step - loss: 0.2900 - accuracy: 0.8992 - val_loss: 0.2820 - val_accuracy: 0.9020 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "1058/1058 [==============================] - 2s 2ms/step - loss: 0.2897 - accuracy: 0.8993 - val_loss: 0.2829 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "1058/1058 [==============================] - 2s 2ms/step - loss: 0.2893 - accuracy: 0.8991 - val_loss: 0.2829 - val_accuracy: 0.9019 - lr: 9.0484e-04\n",
      "Epoch 12/50\n",
      "1058/1058 [==============================] - 2s 2ms/step - loss: 0.2882 - accuracy: 0.8993 - val_loss: 0.2826 - val_accuracy: 0.9023 - lr: 8.1873e-04\n",
      "Epoch 13/50\n",
      "1058/1058 [==============================] - 2s 2ms/step - loss: 0.2901 - accuracy: 0.8983 - val_loss: 0.2828 - val_accuracy: 0.9019 - lr: 7.4082e-04\n",
      "Epoch 14/50\n",
      "1058/1058 [==============================] - 2s 2ms/step - loss: 0.2896 - accuracy: 0.8980 - val_loss: 0.2814 - val_accuracy: 0.9024 - lr: 6.7032e-04\n",
      "Epoch 15/50\n",
      "1058/1058 [==============================] - 2s 2ms/step - loss: 0.2880 - accuracy: 0.8997 - val_loss: 0.2816 - val_accuracy: 0.9025 - lr: 6.0653e-04\n",
      "Epoch 16/50\n",
      "1058/1058 [==============================] - 2s 2ms/step - loss: 0.2872 - accuracy: 0.8991 - val_loss: 0.2818 - val_accuracy: 0.9023 - lr: 5.4881e-04\n",
      "Epoch 17/50\n",
      "1058/1058 [==============================] - 2s 2ms/step - loss: 0.2879 - accuracy: 0.8992 - val_loss: 0.2823 - val_accuracy: 0.9019 - lr: 4.9659e-04\n",
      "Epoch 18/50\n",
      "1058/1058 [==============================] - 2s 2ms/step - loss: 0.2877 - accuracy: 0.8990 - val_loss: 0.2822 - val_accuracy: 0.9020 - lr: 4.4933e-04\n",
      "Epoch 19/50\n",
      "1058/1058 [==============================] - 2s 2ms/step - loss: 0.2878 - accuracy: 0.9000 - val_loss: 0.2821 - val_accuracy: 0.9022 - lr: 4.0657e-04\n",
      "Epoch 20/50\n",
      "1058/1058 [==============================] - 2s 2ms/step - loss: 0.2872 - accuracy: 0.8994 - val_loss: 0.2815 - val_accuracy: 0.9024 - lr: 3.6788e-04\n",
      "Epoch 21/50\n",
      "1058/1058 [==============================] - 2s 2ms/step - loss: 0.2871 - accuracy: 0.9001 - val_loss: 0.2816 - val_accuracy: 0.9025 - lr: 3.3287e-04\n",
      "Epoch 22/50\n",
      "1058/1058 [==============================] - 2s 2ms/step - loss: 0.2869 - accuracy: 0.8998 - val_loss: 0.2817 - val_accuracy: 0.9024 - lr: 3.0119e-04\n",
      "Epoch 23/50\n",
      "1058/1058 [==============================] - 2s 2ms/step - loss: 0.2869 - accuracy: 0.8993 - val_loss: 0.2816 - val_accuracy: 0.9024 - lr: 2.7253e-04\n",
      "Epoch 24/50\n",
      "1058/1058 [==============================] - 2s 2ms/step - loss: 0.2863 - accuracy: 0.8990 - val_loss: 0.2816 - val_accuracy: 0.9019 - lr: 2.4660e-04\n",
      "Epoch 25/50\n",
      "1058/1058 [==============================] - 2s 2ms/step - loss: 0.2876 - accuracy: 0.8981 - val_loss: 0.2815 - val_accuracy: 0.9019 - lr: 2.2313e-04\n",
      "Epoch 26/50\n",
      "1058/1058 [==============================] - 2s 2ms/step - loss: 0.2879 - accuracy: 0.8992 - val_loss: 0.2816 - val_accuracy: 0.9019 - lr: 2.0190e-04\n",
      "Epoch 27/50\n",
      "1058/1058 [==============================] - 2s 2ms/step - loss: 0.2866 - accuracy: 0.9003 - val_loss: 0.2816 - val_accuracy: 0.9024 - lr: 1.8268e-04\n",
      "Epoch 28/50\n",
      "1058/1058 [==============================] - 2s 2ms/step - loss: 0.2873 - accuracy: 0.9003 - val_loss: 0.2818 - val_accuracy: 0.9023 - lr: 1.6530e-04\n",
      "Epoch 29/50\n",
      "1058/1058 [==============================] - 2s 2ms/step - loss: 0.2874 - accuracy: 0.8991 - val_loss: 0.2815 - val_accuracy: 0.9024 - lr: 1.4957e-04\n",
      "Epoch 30/50\n",
      "1058/1058 [==============================] - 2s 2ms/step - loss: 0.2869 - accuracy: 0.9000 - val_loss: 0.2815 - val_accuracy: 0.9020 - lr: 1.3534e-04\n",
      "Epoch 31/50\n",
      "1058/1058 [==============================] - 2s 2ms/step - loss: 0.2877 - accuracy: 0.8989 - val_loss: 0.2816 - val_accuracy: 0.9020 - lr: 1.2246e-04\n",
      "Epoch 32/50\n",
      "1058/1058 [==============================] - 2s 2ms/step - loss: 0.2864 - accuracy: 0.9000 - val_loss: 0.2814 - val_accuracy: 0.9023 - lr: 1.1080e-04\n",
      "Epoch 33/50\n",
      "1058/1058 [==============================] - 2s 2ms/step - loss: 0.2861 - accuracy: 0.9003 - val_loss: 0.2814 - val_accuracy: 0.9022 - lr: 1.0026e-04\n",
      "Epoch 34/50\n",
      "1058/1058 [==============================] - 2s 2ms/step - loss: 0.2859 - accuracy: 0.8997 - val_loss: 0.2813 - val_accuracy: 0.9024 - lr: 9.0718e-05\n",
      "Epoch 35/50\n",
      "1058/1058 [==============================] - 2s 2ms/step - loss: 0.2868 - accuracy: 0.9002 - val_loss: 0.2817 - val_accuracy: 0.9024 - lr: 8.2085e-05\n",
      "Epoch 36/50\n",
      "1058/1058 [==============================] - 2s 2ms/step - loss: 0.2861 - accuracy: 0.9001 - val_loss: 0.2815 - val_accuracy: 0.9024 - lr: 7.4274e-05\n",
      "Epoch 37/50\n",
      "1058/1058 [==============================] - 2s 2ms/step - loss: 0.2865 - accuracy: 0.8990 - val_loss: 0.2816 - val_accuracy: 0.9022 - lr: 6.7206e-05\n",
      "Epoch 38/50\n",
      "1058/1058 [==============================] - 2s 2ms/step - loss: 0.2873 - accuracy: 0.8995 - val_loss: 0.2814 - val_accuracy: 0.9023 - lr: 6.0810e-05\n",
      "Epoch 39/50\n",
      "1058/1058 [==============================] - 2s 2ms/step - loss: 0.2860 - accuracy: 0.9000 - val_loss: 0.2816 - val_accuracy: 0.9024 - lr: 5.5023e-05\n",
      "Epoch 40/50\n",
      "1058/1058 [==============================] - 2s 2ms/step - loss: 0.2862 - accuracy: 0.9002 - val_loss: 0.2814 - val_accuracy: 0.9022 - lr: 4.9787e-05\n",
      "Epoch 41/50\n",
      "1058/1058 [==============================] - 2s 2ms/step - loss: 0.2872 - accuracy: 0.8996 - val_loss: 0.2816 - val_accuracy: 0.9023 - lr: 4.5049e-05\n",
      "Epoch 42/50\n",
      "1058/1058 [==============================] - 2s 2ms/step - loss: 0.2868 - accuracy: 0.8997 - val_loss: 0.2814 - val_accuracy: 0.9023 - lr: 4.0762e-05\n",
      "Epoch 43/50\n",
      "1058/1058 [==============================] - 2s 2ms/step - loss: 0.2856 - accuracy: 0.9003 - val_loss: 0.2814 - val_accuracy: 0.9023 - lr: 3.6883e-05\n",
      "Epoch 44/50\n",
      "1058/1058 [==============================] - 2s 2ms/step - loss: 0.2884 - accuracy: 0.8992 - val_loss: 0.2815 - val_accuracy: 0.9024 - lr: 3.3373e-05\n",
      "Epoch 45/50\n",
      "1058/1058 [==============================] - 2s 2ms/step - loss: 0.2861 - accuracy: 0.9008 - val_loss: 0.2815 - val_accuracy: 0.9022 - lr: 3.0197e-05\n",
      "Epoch 46/50\n",
      "1058/1058 [==============================] - 2s 2ms/step - loss: 0.2862 - accuracy: 0.9007 - val_loss: 0.2816 - val_accuracy: 0.9020 - lr: 2.7324e-05\n",
      "Epoch 47/50\n",
      "1058/1058 [==============================] - 2s 2ms/step - loss: 0.2862 - accuracy: 0.9007 - val_loss: 0.2816 - val_accuracy: 0.9022 - lr: 2.4724e-05\n",
      "Epoch 48/50\n",
      "1058/1058 [==============================] - 2s 2ms/step - loss: 0.2871 - accuracy: 0.9002 - val_loss: 0.2817 - val_accuracy: 0.9020 - lr: 2.2371e-05\n",
      "Epoch 49/50\n",
      "1058/1058 [==============================] - 2s 2ms/step - loss: 0.2865 - accuracy: 0.9008 - val_loss: 0.2816 - val_accuracy: 0.9020 - lr: 2.0242e-05\n",
      "Epoch 50/50\n",
      "1058/1058 [==============================] - 2s 2ms/step - loss: 0.2870 - accuracy: 0.8995 - val_loss: 0.2814 - val_accuracy: 0.9020 - lr: 1.8316e-05\n"
     ]
    }
   ],
   "source": [
    "#学習の実行\n",
    "Learning_and_Predicting(train_df, test_df, features, categorical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Postprocessing\n",
    "\n",
    "def Postprocessing(train_df, test_df):\n",
    "    #最適な閾値を見つける関数\n",
    "    def find_best_threshold_and_score(y_true, y_pred_proba):\n",
    "        best_threshold = 0\n",
    "        best_score = 0\n",
    "        for threshold in np.linspace(0, 1, 1001):\n",
    "            score = f1_score(y_true, y_pred_proba >= threshold, average='macro')\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_threshold = threshold\n",
    "        return best_threshold, best_score\n",
    "    \n",
    "    # ニューラルネットワークモデルの学習データに対する予測確率\n",
    "    nn_model = load_model(f'nn_stacking_model_seed{CFG.seed}_ver{CFG.VER}.h5')\n",
    "    train_pred_proba_nn = nn_model.predict(train_df,oof_combined_features_scaled).flatten()\n",
    "    \n",
    "    # 最適な閾値とスコアを求める\n",
    "    best_threshold_nn, best_score_nn = find_best_threshold_and_score(train_df[CFG.target_col], train_pred_proba_nn)\n",
    "    print(f'NN Best Threshold: {best_threshold_nn}, Best F1 Score: {best_score_nn}')\n",
    "    \n",
    "    # テストデータに対する最終予測\n",
    "    test_pred_proba_nn = nn_model.predict(test_combined_features_scaled).flatten()\n",
    "    test_final_predictions_nn = (test_pred_proba_nn >= best_threshold_nn).astype(int)\n",
    "    # 最終予測結果をコンペ提出用のフォーマットでCSVファイルに出力\n",
    "    submission_df_nn = pd.DataFrame({'Id': test_df.index, 'target': test_final_predictions_nn}).reset_index(drop=True)\n",
    "    submission_df_nn['Id'] = submission_df_nn.index + 4230\n",
    "    submission_df_nn.to_csv(f'stacking_nn_submission_best_score{best_score_nn:.4f}_seed{CFG.seed}_ver{CFG.VER}_{CFG.AUTHOR}.csv', header=False, index=False)\n",
    "    \n",
    "    \n",
    "    # ロジスティック回帰モデルの学習データに対する予測確率\n",
    "    lr_model = pickle.load(open(f'lr_stacking_model_seed{CFG.seed}_ver{CFG.VER}.pkl','rb'))\n",
    "    train_pred_proba_lr = lr_model.predict_proba(oof_combined_features_scaled)[:, 1]\n",
    "    \n",
    "    # 最適な閾値とスコアを求める\n",
    "    best_threshold_lr, best_score_lr = find_best_threshold_and_score(train_df[CFG.target_col], train_pred_proba_nn)\n",
    "    print(f'LR Best Threshold: {best_threshold_lr}, Best F1 Score: {best_score_lr}')\n",
    "    \n",
    "    # テストデータに対する最終予測\n",
    "    test_pred_proba_lr = lr_model.predict_proba(test_combined_features_scaled)[:, 1]\n",
    "    test_final_predictions_lr = (test_pred_proba_lr >= best_threshold_lr).astype(int)\n",
    "    # 最終予測結果をコンペ提出用のフォーマットでCSVファイルに出力\n",
    "    submission_df_lr = pd.DataFrame({'Id': test_df.index, 'target': test_final_predictions_lr}).reset_index(drop=True)\n",
    "    submission_df_lr['Id'] = submission_df_lr.index + 4230\n",
    "    submission_df_lr.to_csv(f'stacking_lr_submission_best_score{best_score_lr:.4f}_seed{CFG.seed}_ver{CFG.VER}_{CFG.AUTHOR}.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Postprocessing() takes 0 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15284/3766098792.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#予測の実行\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mPostprocessing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcategorical_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: Postprocessing() takes 0 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "#予測の実行\n",
    "Postprocessing(train_df, test_df)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "13858cd0c554419bb867071b5c810b30": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4074a8c3b85548ed9424375a19416c7d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "573ab758539147f1a2bc21ac9f0347ce": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5de7c5cebd814e1caa8437775031ea12": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6762ef5aab7e4d18b98e7d837dedc03f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6f14799525cf44158bbffc0e9a08891b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b7dceec9910a46cea90915015ca58358",
      "placeholder": "​",
      "style": "IPY_MODEL_573ab758539147f1a2bc21ac9f0347ce",
      "value": "100%"
     }
    },
    "76ddf1599bb144b2a6b4573027c6f662": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4074a8c3b85548ed9424375a19416c7d",
      "placeholder": "​",
      "style": "IPY_MODEL_13858cd0c554419bb867071b5c810b30",
      "value": " 1000/1000 [00:19&lt;00:00, 37.91it/s]"
     }
    },
    "b0ab4a4e03b242e2ae3610b6a52b5c14": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b7dceec9910a46cea90915015ca58358": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d239d2ff297a4c00aa350f8699ca6bb1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5de7c5cebd814e1caa8437775031ea12",
      "max": 1000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b0ab4a4e03b242e2ae3610b6a52b5c14",
      "value": 1000
     }
    },
    "ef58b4abcf0f43cd90a523a3875a2f28": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6f14799525cf44158bbffc0e9a08891b",
       "IPY_MODEL_d239d2ff297a4c00aa350f8699ca6bb1",
       "IPY_MODEL_76ddf1599bb144b2a6b4573027c6f662"
      ],
      "layout": "IPY_MODEL_6762ef5aab7e4d18b98e7d837dedc03f"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
