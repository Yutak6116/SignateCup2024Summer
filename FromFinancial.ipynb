{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "B3WEPx1JJqlG"
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Library\n",
    "# ====================================================\n",
    "import sys\n",
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import random\n",
    "import re\n",
    "import unicodedata\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import pickle\n",
    "import itertools\n",
    "from tqdm.auto import tqdm\n",
    "import category_encoders as ce\n",
    "import torch\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, train_test_split, GroupKFold\n",
    "from sklearn.metrics import log_loss, roc_auc_score, matthews_corrcoef, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import Pool, CatBoostRegressor, CatBoostClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Activation\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import Callback\n",
    "from keras.models import clone_model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "pd.set_option('display.max_columns',1000)\n",
    "pd.set_option('display.max_rows',100)\n",
    "\n",
    "# ====================================================\n",
    "# Configurations\n",
    "# ====================================================\n",
    "class CFG:\n",
    "    VER = 100\n",
    "    AUTHOR = 'Yuta.K'\n",
    "    COMPETITION = 'SignateCup2024Summer'\n",
    "    DATA_PATH = Path('dataset')\n",
    "    OOF_DATA_PATH = Path('oof')\n",
    "    MODEL_DATA_PATH = Path('models')\n",
    "    SUB_DATA_PATH = Path('submission')\n",
    "    METHOD_LIST = ['lightgbm', 'xgboost', 'catboost']\n",
    "#     METHOD_LIST = [ 'adaboost','lightgbm', 'xgboost', 'catboost']\n",
    "    seed = 42\n",
    "    n_folds = 2\n",
    "    target_col = 'ProdTaken'\n",
    "    metric = 'auc'\n",
    "    metric_maximize_flag = True\n",
    "    num_boost_round = 500\n",
    "    early_stopping_round = 200\n",
    "    verbose = 25\n",
    "    classification_lgb_params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'auc',\n",
    "        'learning_rate': 0.05,\n",
    "        'seed': seed,\n",
    "    }\n",
    "    classification_xgb_params = {\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'auc',\n",
    "        'learning_rate': 0.05,\n",
    "        'random_state': seed,\n",
    "    }\n",
    "    classification_cat_params = {\n",
    "        'learning_rate': 0.05,\n",
    "        'iterations': num_boost_round,\n",
    "        'random_seed': seed,\n",
    "    }\n",
    "    classification_adaboost_params = {\n",
    "        'n_estimators': 100,\n",
    "        'learning_rate': 1.0,\n",
    "        'random_state': 42,\n",
    "    }\n",
    "    \n",
    "# ====================================================\n",
    "# Seed everything\n",
    "# ====================================================\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "seed_everything(CFG.seed)\n",
    "\n",
    "\n",
    "# ====================================================\n",
    "# Metric\n",
    "# ====================================================\n",
    "# AUC\n",
    "\n",
    "# ====================================================\n",
    "# LightGBM Metric\n",
    "# ====================================================\n",
    "def lgb_metric(y_pred, y_true):\n",
    "    y_true = y_true.get_label()\n",
    "    return 'auc', roc_auc_score(y_true, y_pred), CFG.metric_maximize_flag\n",
    "\n",
    "# ====================================================\n",
    "# XGBoost Metric\n",
    "# ====================================================\n",
    "def xgb_metric(y_pred, y_true):\n",
    "    y_true = y_true.get_label()\n",
    "    return 'auc', roc_auc_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "g6R4KoxhL91E"
   },
   "outputs": [],
   "source": [
    "#ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿\n",
    "train_df = pd.read_csv(f'{CFG.DATA_PATH}/train.csv', index_col=0)\n",
    "test_df = pd.read_csv(f'{CFG.DATA_PATH}/test.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "El2B8eayMmyZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing finished\n"
     ]
    }
   ],
   "source": [
    "def preprocessing(train_df, test_df):\n",
    "    # Age numeric\n",
    "    # æ¼¢æ•°å­—ã¨ã‚¢ãƒ©ãƒ“ã‚¢æ•°å­—ã®ãƒãƒƒãƒ”ãƒ³ã‚°\n",
    "    kanji_to_num = {'ä¸€': 1, 'äºŒ': 2, 'ä¸‰': 3, 'å››': 4, 'äº”': 5, 'å…­': 6, 'ä¸ƒ': 7, 'å…«': 8, 'ä¹': 9,'å': 10, 'ç™¾': 100, 'åƒ': 1000, 'ä¸‡': 10000,'é›¶': 0, 'ã€‡': 0}\n",
    "    def kanji_to_arabic(kanji):\n",
    "        result = 0\n",
    "        temp = 0\n",
    "        for char in kanji:\n",
    "            value = kanji_to_num.get(char, None)\n",
    "            if value is not None:\n",
    "                if value < 10:\n",
    "                    if temp == 0:\n",
    "                        temp = value\n",
    "                    else:\n",
    "                        temp = temp * 10 + value\n",
    "                elif value >= 10:\n",
    "                    if temp == 0:\n",
    "                        temp = 1\n",
    "                    result += temp * value\n",
    "                    temp = 0\n",
    "        return result + temp\n",
    "    def process_age(age):\n",
    "        if age is None or str(age) == 'nan':\n",
    "            return None\n",
    "        age = unicodedata.normalize('NFKC', age)\n",
    "        age = ''.join([c for c in age if c.isdigit() or c in kanji_to_num])\n",
    "        if age.isdigit():\n",
    "            return int(age)\n",
    "        return kanji_to_arabic(age)\n",
    "\n",
    "    # TypeofContact categorical(dummy)\n",
    "    def TypeofContact_to_dummy(str):\n",
    "        if str == 'Self Enquiry':\n",
    "            return 1\n",
    "        elif str == 'Company Invited':\n",
    "            return 0\n",
    "        \n",
    "    # CityTier é †åºå°ºåº¦\n",
    "    \n",
    "    # DurationOfPitch numeric\n",
    "    def convert_to_minutes(duration):\n",
    "        # durationãŒfloatå‹ã¾ãŸã¯Noneã§ã‚ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ã€æ–‡å­—åˆ—ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª\n",
    "        if pd.isnull(duration):\n",
    "            return None  # NaNã®å ´åˆã€Noneã‚’è¿”ã™\n",
    "        duration = str(duration)  # æ–‡å­—åˆ—ã«å¤‰æ›ã—ã¦ã‚¨ãƒ©ãƒ¼ã‚’é˜²ã\n",
    "        if 'åˆ†' in duration:\n",
    "            return float(duration.replace('åˆ†', ''))\n",
    "        elif 'ç§’' in duration:\n",
    "            return float(duration.replace('ç§’', '')) / 60  # ç§’ã‚’åˆ†ã«å¤‰æ›ã—ã€æ•´æ•°ã§è¿”ã™\n",
    "\n",
    "    # Occupation categorical\n",
    "    def Occupation_to_dummy(str):\n",
    "        if str == 'Large Business':\n",
    "            return 2\n",
    "        elif str == 'Small Business':\n",
    "            return 1\n",
    "        elif str == 'Salaried':\n",
    "            return 0\n",
    "       \n",
    "    # Gender categorical\n",
    "    def Gender_dealing(gender):\n",
    "        # æ–‡å­—åˆ—ã‚’åŠè§’ã«å¤‰æ›ã—ã€å¤§æ–‡å­—ã«çµ±ä¸€\n",
    "        gender = unicodedata.normalize('NFKC', gender).upper().strip()\n",
    "        # ä¸è¦ãªç©ºç™½ã‚’å‰Šé™¤\n",
    "        gender = ''.join(gender.split())\n",
    "\n",
    "        if 'FEMALE' in gender:\n",
    "            return 1\n",
    "        elif 'MALE' in gender:\n",
    "            return 0\n",
    "        else:\n",
    "            return None  # æ€§åˆ¥ãŒè­˜åˆ¥ã§ããªã„å ´åˆã¯Noneã‚’è¿”ã™\n",
    "        \n",
    "    # NumberOfPersonVisiting numeric\n",
    "    \n",
    "    # NumberOfFollowups numeric\n",
    "    def NumberOfFollowups_dealing(input_int):\n",
    "        if input_int >= 100:\n",
    "            return input_int /100\n",
    "        else:\n",
    "            return input_int\n",
    "    \n",
    "    # ProductPitched categorical\n",
    "    # Designation categorical\n",
    "    def standardize_str(input_str):\n",
    "        # æ–‡å­—åˆ—ã‚’åŠè§’ã«å¤‰æ›ã—ã€å°æ–‡å­—ã«çµ±ä¸€\n",
    "        input_str = unicodedata.normalize('NFKC', input_str).lower().strip()\n",
    "        # ä¸è¦ãªç©ºç™½ã‚„ç‰¹æ®Šè¨˜å·ã‚’å‰Šé™¤\n",
    "        input_str = ''.join(input_str.split())\n",
    "        input_str = input_str.replace('|', 'l').replace('Ã—', 'x').replace('ğ˜¤', 'c').replace('ğ–º', 'a').replace('ğ™³', 'd')\n",
    "        # ãã®ä»–ç‰¹æ®Šæ–‡å­—ã‚’é€šå¸¸ã®è‹±å­—ã«ç½®æ›\n",
    "        input_str = input_str.replace('á—', 'd').replace('ğŠ¡', 'a').replace('ğ˜³', 'r').replace('ê“¢', 's').replace('Ä±', 'i')\n",
    "        input_str = input_str.replace('Î²', 'b').replace('Ğ²', 'b').replace('Ñ', 'c').replace('Õ¿', 's').replace('Ï‚', 'c')\n",
    "        input_str = input_str.replace('ê­°', 'd').replace('Îµ', 'e').replace('Î¹', 'i').replace('Î±', 'a').replace('Õ¸', 'n')\n",
    "        input_str = input_str.replace('Ñ•', 's').replace('Î¼', 'm').replace('Ğµ', 'e').replace('Ğ°', 'a').replace('Ñµ', 'v')\n",
    "        input_str = input_str.replace('aasic', 'basic')\n",
    "        return input_str\n",
    "    \n",
    "    # PreferredPropertyStar é †åºå°ºåº¦\n",
    "    \n",
    "    # NumberOfTrips numeric\n",
    "    def NumberOfTrips_dealing(str):\n",
    "        if pd.isnull(str):\n",
    "            return None \n",
    "        if 'åŠå¹´ã«' in str:\n",
    "            return 2 * int(str.replace('åŠå¹´ã«', '').replace('å›', ''))\n",
    "        elif 'å¹´ã«' in str:\n",
    "            return int(str.replace('å¹´ã«', '').replace('å›', ''))\n",
    "        elif 'å››åŠæœŸã«' in str:\n",
    "            return 4 * int(str.replace('å››åŠæœŸã«', '').replace('å›', ''))\n",
    "        else :\n",
    "            return int(str)\n",
    "        \n",
    "    # Passport categorical(dummy)\n",
    "    \n",
    "    # PitchSatisfactionScore é †åºå°ºåº¦ã ã‘ã©é–“éš”å°ºåº¦çš„è¦ç´ ã‚ã‚Š\n",
    "    \n",
    "    # MonthlyIncome numeric\n",
    "    def MonthlyIncome_dealing(input_str):\n",
    "        if pd.isnull(input_str):\n",
    "            return None \n",
    "        if 'æœˆå' in input_str:\n",
    "            return 10000 * float(input_str.replace('æœˆå', '').replace('ä¸‡å††', ''))\n",
    "        elif 'ä¸‡å††' in input_str:\n",
    "            return 10000 * float(input_str.replace('ä¸‡å††', ''))\n",
    "        else:\n",
    "            return float(input_str)\n",
    "        \n",
    "    # customer_info\n",
    "    def customer_info_dealing(input_str):\n",
    "        # æ–‡å­—åˆ—ã‚’åŠè§’ã«å¤‰æ›ã—ã€å°æ–‡å­—ã«çµ±ä¸€\n",
    "        input_str = unicodedata.normalize('NFKC', input_str).lower().strip()\n",
    "        # ä¸è¦ãªç©ºç™½ã‚„ç‰¹æ®Šè¨˜å·ã‚’å‰Šé™¤\n",
    "        input_str = input_str.replace('/', ' ').replace('ï¼', ' ').replace('ã€', ' ').replace('ã€€', ' ')\n",
    "        input_str = input_str.replace('\\u3000', ' ').replace('\\t', ' ').replace('\\n', ' ')\n",
    "        input_str = re.sub(r'(?<=\\S)\\s+(?=\\S)', ',', input_str, count=2)\n",
    "        return input_str\n",
    "    \n",
    "    # married categorical\n",
    "    \n",
    "    # car_possesion categorival(dummy)\n",
    "    def car_possesion_dealing(input_str):\n",
    "        if input_str in ['è»Šæœªæ‰€æŒ', 'è‡ªå‹•è»Šæœªæ‰€æœ‰', 'è‡ªå®¶ç”¨è»Šãªã—', 'ä¹—ç”¨è»Šãªã—', 'è»Šãªã—', 'è»Šä¿æœ‰ãªã—', 0]:\n",
    "            return 0\n",
    "        elif input_str in ['è»Šæ‰€æŒ', 'è‡ªå‹•è»Šæ‰€æœ‰', 'è‡ªå®¶ç”¨è»Šã‚ã‚Š', 'ä¹—ç”¨è»Šæ‰€æŒ', 'è»Šä¿æœ‰', 'è»Šã‚ã‚Š', 1]:\n",
    "            return 1\n",
    "        \n",
    "    # offspring -1ä»¥å¤–ã¯numeric\n",
    "    def offspring_dealing(input_str):\n",
    "        if '1' in input_str:\n",
    "            return 1\n",
    "        elif '2' in input_str:\n",
    "            return 2\n",
    "        elif '3' in input_str:\n",
    "            return 3\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    def offspring_identified_dealing(input_str):\n",
    "        if input_str in ['å­ä¾›ã®æ•°ä¸æ˜', 'ä¸æ˜', 'ã‚ã‹ã‚‰ãªã„', 'å­è‚²ã¦çŠ¶æ³ä¸æ˜', 'å­ã®æ•°ä¸è©³']:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    def dealing_missing_values(input_df):\n",
    "        df = input_df.copy()\n",
    "        df['Age'] = df['Age'].fillna(df['Age'].mean())\n",
    "        df['TypeofContact'] = df['TypeofContact'].fillna(df['TypeofContact'].median())\n",
    "        df['DurationOfPitch'] = df['DurationOfPitch'].fillna(df['DurationOfPitch'].mean())\n",
    "        df['NumberOfFollowups'] = df['NumberOfFollowups'].fillna(df['NumberOfFollowups'].mean())\n",
    "        df['NumberOfTrips'] = df['NumberOfTrips'].fillna(df['NumberOfTrips'].mean())\n",
    "        df['MonthlyIncome'] = df['MonthlyIncome'].fillna(df['MonthlyIncome'].mean())\n",
    "        return df\n",
    "    \n",
    "    def dummy_ex(feature, train_df, test_df):\n",
    "        # OneHotEncoder ã®åˆæœŸåŒ–æ™‚ã« sparse_output å¼•æ•°ã‚’ä½¿ç”¨\n",
    "        ohe = OneHotEncoder(sparse_output=False, categories='auto')\n",
    "        new_array = pd.concat([train_df[[feature]], test_df[[feature]]], axis=0)\n",
    "        ohe.fit(new_array)\n",
    "\n",
    "        # ãƒ€ãƒŸãƒ¼å¤‰æ•°ã®åˆ—åã®ä½œæˆ\n",
    "        columns = [f'{feature}_{v}' for v in ohe.categories_[0]]\n",
    "\n",
    "        # ç”Ÿæˆã•ã‚ŒãŸãƒ€ãƒŸãƒ¼å¤‰æ•°ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«å¤‰æ›\n",
    "        dummy_vals_train = pd.DataFrame(ohe.transform(train_df[[feature]]), columns=columns)\n",
    "        dummy_vals_test = pd.DataFrame(ohe.transform(test_df[[feature]]), columns=columns)\n",
    "\n",
    "        # æ®‹ã‚Šã®å¤‰æ•°ã¨çµåˆ\n",
    "        tr = pd.concat([train_df.drop([feature], axis=1), dummy_vals_train.reset_index(drop=True)], axis=1)\n",
    "        te = pd.concat([test_df.drop([feature], axis=1), dummy_vals_test.reset_index(drop=True)], axis=1)\n",
    "\n",
    "        return tr, te\n",
    "    \n",
    "    def function_apply(input_df):\n",
    "        df = input_df.copy()\n",
    "        df['Age'] = df['Age'].apply(process_age)\n",
    "        df['TypeofContact'] = df['TypeofContact'].apply(TypeofContact_to_dummy)\n",
    "        df['DurationOfPitch'] = df['DurationOfPitch'].apply(convert_to_minutes)\n",
    "        df['Occupation'] = df['Occupation'].apply(Occupation_to_dummy)\n",
    "        df['Gender'] = df['Gender'].apply(Gender_dealing)\n",
    "        df['NumberOfFollowups'] = df['NumberOfFollowups'].apply(NumberOfFollowups_dealing)\n",
    "        df['ProductPitched'] = df['ProductPitched'].apply(standardize_str)\n",
    "        df['NumberOfTrips'] = df['NumberOfTrips'].apply(NumberOfTrips_dealing)\n",
    "        df['Designation'] = df['Designation'].apply(standardize_str)\n",
    "        df['MonthlyIncome'] = df['MonthlyIncome'].apply(MonthlyIncome_dealing)\n",
    "        df['customer_info'] = df['customer_info'].apply(customer_info_dealing)\n",
    "        df[['married', 'car_possesion', 'offspring']] = df['customer_info'].str.split(',', n=2, expand=True)\n",
    "        df = df.drop(['customer_info'],axis=1)\n",
    "        df['car_possesion'] = df['car_possesion'].apply(car_possesion_dealing)\n",
    "        df['offspring'] = df['offspring'].apply(offspring_dealing)\n",
    "        df['offspring_identified'] = df['offspring'].apply(offspring_identified_dealing)\n",
    "        df = dealing_missing_values(df)\n",
    "        return df\n",
    "    \n",
    "    dummy_col = ['CityTier', 'Occupation', 'ProductPitched', 'PreferredPropertyStar', 'PitchSatisfactionScore', 'Designation', 'married']\n",
    "    std_feature = ['Age', 'DurationOfPitch', 'NumberOfPersonVisiting', 'NumberOfFollowups', 'NumberOfTrips', 'MonthlyIncome']\n",
    "    \n",
    "    def function_apply_both(train_df, test_df, std_feature):\n",
    "        tr_df = train_df.copy()\n",
    "        te_df = test_df.copy()\n",
    "        tr_df = function_apply(tr_df)\n",
    "        te_df = function_apply(te_df)\n",
    "        for feature in dummy_col:\n",
    "            tr_df, te_x = dummy_ex(feature, tr_df, te_df)\n",
    "        std_sc = StandardScaler()\n",
    "        tr_df[std_feature] = std_sc.fit_transform(tr_df[std_feature])\n",
    "        te_df[std_feature] = std_sc.fit_transform(te_df[std_feature])\n",
    "        return tr_df, te_df\n",
    "    \n",
    "    tr_df, te_df = function_apply_both(train_df, test_df, std_feature)\n",
    "    \n",
    "    print('Preprocessing finished')\n",
    "    return tr_df, te_df\n",
    "\n",
    "train_df, test_df = preprocessing(train_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "quaQcTgQOjyJ"
   },
   "outputs": [],
   "source": [
    "#Learning & Predicting\n",
    "\n",
    "#1æ®µéšç›®ã®å­¦ç¿’\n",
    "def Pre_Learning(train_df,test_df, features, categorical_features):\n",
    "    \n",
    "    #adaboostã§ã®å­¦ç¿’ãƒ¡ã‚½ãƒƒãƒ‰ã®å®šç¾©\n",
    "    def adaboost_training(x_train, y_train, x_valid, y_valid, features, categorical_features):\n",
    "        model = AdaBoostClassifier(**CFG.classification_adaboost_params)\n",
    "        model.fit(x_train, y_train)\n",
    "        # Predict validation\n",
    "        valid_pred = model.predict_proba(x_valid)[:, 1]\n",
    "        return model, valid_pred\n",
    "\n",
    "    #lightgbmã§ã®å­¦ç¿’ãƒ¡ã‚½ãƒƒãƒ‰ã®å®šç¾©\n",
    "    def lightgbm_training(x_train, y_train, x_valid, y_valid, features, categorical_features):\n",
    "        lgb_train = lgb.Dataset(x_train, y_train, categorical_feature=categorical_features)\n",
    "        lgb_valid = lgb.Dataset(x_valid, y_valid, categorical_feature=categorical_features)\n",
    "        model = lgb.train(\n",
    "                    params = CFG.classification_lgb_params,\n",
    "                    train_set = lgb_train,\n",
    "                    num_boost_round = CFG.num_boost_round,\n",
    "                    valid_sets = [lgb_train, lgb_valid],\n",
    "                    feval = lgb_metric,\n",
    "                    callbacks=[lgb.early_stopping(stopping_rounds=CFG.early_stopping_round,\n",
    "                                                  verbose=CFG.verbose)]\n",
    "                )\n",
    "        # Predict validation\n",
    "        valid_pred = model.predict(x_valid)\n",
    "        return model, valid_pred\n",
    "\n",
    "    #xgboostã§ã®å­¦ç¿’ãƒ¡ã‚½ãƒƒãƒ‰ã®å®šç¾©\n",
    "    def xgboost_training(x_train, y_train, x_valid, y_valid, features, categorical_features):\n",
    "        xgb_train = xgb.DMatrix(data=x_train, label=y_train)\n",
    "        xgb_valid = xgb.DMatrix(data=x_valid, label=y_valid)\n",
    "        model = xgb.train(\n",
    "                    CFG.classification_xgb_params,\n",
    "                    dtrain = xgb_train,\n",
    "                    num_boost_round = CFG.num_boost_round,\n",
    "                    evals = [(xgb_train, 'train'), (xgb_valid, 'eval')],\n",
    "                    early_stopping_rounds = CFG.early_stopping_round,\n",
    "                    verbose_eval = CFG.verbose,\n",
    "                    feval = xgb_metric,\n",
    "                    maximize = CFG.metric_maximize_flag,\n",
    "                )\n",
    "        # Predict validation\n",
    "        valid_pred = model.predict(xgb.DMatrix(x_valid))\n",
    "        return model, valid_pred\n",
    "\n",
    "    #catboostã§ã®å­¦ç¿’ãƒ¡ã‚½ãƒƒãƒ‰ã®å®šç¾©\n",
    "    def catboost_training(x_train, y_train, x_valid, y_valid, features, categorical_features):\n",
    "        cat_train = Pool(data=x_train, label=y_train, cat_features=categorical_features)\n",
    "        cat_valid = Pool(data=x_valid, label=y_valid, cat_features=categorical_features)\n",
    "        model = CatBoostClassifier(**CFG.classification_cat_params)\n",
    "        model.fit(cat_train,\n",
    "                  eval_set = [cat_valid],\n",
    "                  early_stopping_rounds = CFG.early_stopping_round,\n",
    "                  verbose = CFG.verbose,\n",
    "                  use_best_model = True)\n",
    "        # Predict validation\n",
    "        valid_pred = model.predict_proba(x_valid)[:, 1]\n",
    "        return model, valid_pred\n",
    "\n",
    "    #ä»»æ„ã®ãƒ¢ãƒ‡ãƒ«ã§ã®ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³å­¦ç¿’ãƒ¡ã‚½ãƒƒãƒ‰ã®å®šç¾©\n",
    "    def gradient_boosting_model_cv_training(method, train_df, features, categorical_features):\n",
    "        # Create a numpy array to store out of folds predictions\n",
    "        oof_predictions = np.zeros(len(train_df))\n",
    "        oof_fold = np.zeros(len(train_df))\n",
    "        kfold = KFold(n_splits=CFG.n_folds, shuffle=True, random_state=CFG.seed)\n",
    "        for fold, (train_index, valid_index) in enumerate(kfold.split(train_df)):\n",
    "            print('-'*50)\n",
    "            print(f'{method} training fold {fold+1}')\n",
    "\n",
    "            x_train = train_df[features].iloc[train_index]\n",
    "            y_train = train_df[CFG.target_col].iloc[train_index]\n",
    "            x_valid = train_df[features].iloc[valid_index]\n",
    "            y_valid = train_df[CFG.target_col].iloc[valid_index]\n",
    "\n",
    "            model = None  # ãƒ¢ãƒ‡ãƒ«å¤‰æ•°ã‚’åˆæœŸåŒ–ã™ã‚‹\n",
    "            valid_pred = None\n",
    "\n",
    "            if method == 'adaboost':\n",
    "                model, valid_pred = adaboost_training(x_train, y_train, x_valid, y_valid, features, categorical_features)\n",
    "            if method == 'lightgbm':\n",
    "                model, valid_pred = lightgbm_training(x_train, y_train, x_valid, y_valid, features, categorical_features)\n",
    "            if method == 'xgboost':\n",
    "                model, valid_pred = xgboost_training(x_train, y_train, x_valid, y_valid, features, categorical_features)\n",
    "            if method == 'catboost':\n",
    "                model, valid_pred = catboost_training(x_train, y_train, x_valid, y_valid, features, categorical_features)  \n",
    "            # Save best model\n",
    "            pickle.dump(model, open(f'{method}_fold{fold + 1}_seed{CFG.seed}_ver{CFG.VER}.pkl', 'wb'))\n",
    "            # Add to out of folds array\n",
    "            oof_predictions[valid_index] = valid_pred\n",
    "            oof_fold[valid_index] = fold + 1\n",
    "            del x_train, x_valid, y_train, y_valid, model, valid_pred\n",
    "            gc.collect()\n",
    "\n",
    "        # Compute out of folds metric\n",
    "        score = f1_score(train_df[CFG.target_col], oof_predictions >= 0.5, average='macro')\n",
    "        print(f'{method} our out of folds CV f1score is {score}')\n",
    "        # Create a dataframe to store out of folds predictions\n",
    "        oof_df = pd.DataFrame({CFG.target_col: train_df[CFG.target_col], f'{method}_prediction': oof_predictions, 'fold': oof_fold})\n",
    "        oof_df.to_csv(f'oof_{method}_seed{CFG.seed}_ver{CFG.VER}.csv', index = False)\n",
    "\n",
    "    #adaboostã®å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿é–¢æ•°\n",
    "    def adaboost_inference(x_test):\n",
    "        test_pred = np.zeros(len(x_test))\n",
    "        for fold in range(CFG.n_folds):\n",
    "            model = pickle.load(open(f'adaboost_fold{fold + 1}_seed{CFG.seed}_ver{CFG.VER}.pkl', 'rb'))\n",
    "            # Predict\n",
    "            pred = model.predict_proba(x_test)[:, 1]\n",
    "            test_pred += pred\n",
    "        return test_pred / CFG.n_folds\n",
    "\n",
    "    #lightgbmã®å­¦ç¿’ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿é–¢æ•°\n",
    "    def lightgbm_inference(x_test):\n",
    "        test_pred = np.zeros(len(x_test))\n",
    "        for fold in range(CFG.n_folds):\n",
    "            model = pickle.load(open(f'lightgbm_fold{fold + 1}_seed{CFG.seed}_ver{CFG.VER}.pkl', 'rb'))\n",
    "            # Predict\n",
    "            pred = model.predict(x_test)\n",
    "            test_pred += pred\n",
    "        return test_pred / CFG.n_folds\n",
    "\n",
    "    #xgboostã®å­¦ç¿’ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿é–¢æ•°\n",
    "    def xgboost_inference(x_test):\n",
    "        test_pred = np.zeros(len(x_test))\n",
    "        for fold in range(CFG.n_folds):\n",
    "            model = pickle.load(open(f'xgboost_fold{fold + 1}_seed{CFG.seed}_ver{CFG.VER}.pkl', 'rb'))\n",
    "            # Predict\n",
    "            pred = model.predict(xgb.DMatrix(x_test))\n",
    "            test_pred += pred\n",
    "        return test_pred / CFG.n_folds\n",
    "\n",
    "    #catboostã®å­¦ç¿’ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿é–¢æ•°\n",
    "    def catboost_inference(x_test):\n",
    "        test_pred = np.zeros(len(x_test))\n",
    "        for fold in range(CFG.n_folds):\n",
    "            model = pickle.load(open(f'catboost_fold{fold + 1}_seed{CFG.seed}_ver{CFG.VER}.pkl', 'rb'))\n",
    "            # Predict\n",
    "            pred = model.predict_proba(x_test)[:, 1]\n",
    "            test_pred += pred\n",
    "        return test_pred / CFG.n_folds\n",
    "\n",
    "    #ä»»æ„ã®ãƒ¡ã‚½ãƒƒãƒ‰ã«å¯¾ã—ã¦äºˆæ¸¬ã‚’è¿”ã™é–¢æ•°\n",
    "    def gradient_boosting_model_inference(method, test_df, features, categorical_features):\n",
    "        x_test = test_df[features]\n",
    "        if method == 'adaboost':\n",
    "            test_pred = adaboost_inference(x_test)\n",
    "        if method == 'lightgbm':\n",
    "            test_pred = lightgbm_inference(x_test)\n",
    "        if method == 'xgboost':\n",
    "            test_pred = xgboost_inference(x_test)\n",
    "        if method == 'catboost':\n",
    "            test_pred = catboost_inference(x_test)\n",
    "        return test_pred\n",
    "\n",
    "    for method in CFG.METHOD_LIST:\n",
    "        gradient_boosting_model_cv_training(method, train_df, features, categorical_features)\n",
    "        test_df[f'{method}_pred_prob'] = gradient_boosting_model_inference(method, test_df, features, categorical_features)\n",
    "        \n",
    "        \n",
    "#2æ®µéšç›®ã®å­¦ç¿’ã€€ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«ã‚ˆã‚‹ã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°\n",
    "def Post_Learning(train_df,test_df):\n",
    "    #ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ¢ãƒ‡ãƒ«ä½œæˆé–¢æ•°\n",
    "    def create_nn_model(input_shape):\n",
    "        model = Sequential([\n",
    "            Dense(64, input_shape=(input_shape,)),\n",
    "            BatchNormalization(),\n",
    "            Activation('relu'),\n",
    "            Dropout(0.5),\n",
    "\n",
    "            Dense(32),\n",
    "            BatchNormalization(),\n",
    "            Activation('relu'),\n",
    "            Dropout(0.5),\n",
    "\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        optimizer = Adam(lr=0.001)\n",
    "        model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    #ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ç”¨å­¦ç¿’ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼\n",
    "    def scheduler(epoch, lr):\n",
    "            if epoch < 10:\n",
    "                return lr\n",
    "            else:\n",
    "                return lr * np.exp(-0.1)\n",
    "\n",
    "    #ç‰¹å¾´é‡åŒå£«ã§ç©ã‚’ä½œã‚‹é–¢æ•°\n",
    "    def create_interaction_features(features):\n",
    "            n_features = features.shape[1]\n",
    "            interaction_features = []\n",
    "            for i in range(n_features):\n",
    "                for j in range(i + 1, n_features):\n",
    "                    interaction_features.append(features[:, i] * features[:, j])  \n",
    "            return np.column_stack(interaction_features)\n",
    "    \n",
    "    # OOFäºˆæ¸¬ã‚’åŸºã«æ–°ãŸãªç‰¹å¾´é‡ã‚’ä½œæˆ\n",
    "    oof_features = np.zeros((train_df.shape[0], len(CFG.METHOD_LIST)))\n",
    "    for i, method in enumerate(CFG.METHOD_LIST):\n",
    "        oof_df = pd.read_csv(f'oof_{method}_seed{CFG.seed}_ver{CFG.VER}.csv')\n",
    "        oof_features[:, i] = oof_df[f'{method}_prediction']\n",
    "    \n",
    "    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®äºˆæ¸¬ã‚’åŸºã«ç‰¹å¾´é‡ã‚’ä½œæˆ\n",
    "    test_features = np.zeros((test_df.shape[0], len(CFG.METHOD_LIST)))\n",
    "    for i, method in enumerate(CFG.METHOD_LIST):\n",
    "        test_features[:, i] = test_df[f'{method}_pred_prob']\n",
    "\n",
    "    # ç‰¹å¾´é‡åŒå£«ã®ç©ã‚’è¿½åŠ \n",
    "    oof_interaction_features = create_interaction_features(oof_features)\n",
    "    test_interaction_features = create_interaction_features(test_features)\n",
    "\n",
    "    # å…ƒã®ç‰¹å¾´é‡ã¨ç›¸äº’ä½œç”¨ç‰¹å¾´é‡ã‚’çµ„ã¿åˆã‚ã›\n",
    "    oof_combined_features = np.hstack([oof_features, oof_interaction_features])\n",
    "    test_combined_features = np.hstack([test_features, test_interaction_features])\n",
    "\n",
    "    # ç‰¹å¾´é‡ã®æ¨™æº–åŒ–\n",
    "    global oof_combined_features_scaled, test_combined_features_scaled\n",
    "    scaler = StandardScaler()\n",
    "    oof_combined_features_scaled = scaler.fit_transform(oof_combined_features)\n",
    "    test_combined_features_scaled = scaler.transform(test_combined_features)   \n",
    "    \n",
    "    # ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’\n",
    "    nn_model = create_nn_model(oof_combined_features_scaled.shape[1])\n",
    "    callbacks_list = [LearningRateScheduler(scheduler)]\n",
    "    nn_model.fit(oof_combined_features_scaled, train_df[CFG.target_col],\n",
    "                 validation_split=0.2, epochs=50, batch_size=32, callbacks=callbacks_list, verbose=1)\n",
    "    nn_model.save(f'nn_stacking_model_seed{CFG.seed}_ver{CFG.VER}.h5')\n",
    "    \n",
    "    #ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’\n",
    "    lr_model = LogisticRegression()\n",
    "    lr_model.fit(oof_combined_features_scaled, train_df[CFG.target_col])\n",
    "    pickle.dump(lr_model, open(f'lr_stacking_model_seed{CFG.seed}_ver{CFG.VER}.pkl','wb'))\n",
    "\n",
    "def Learning_and_Predicting(train_df, test_df, features, categorical_features):\n",
    "    Pre_Learning(train_df, test_df, features, categorical_features)\n",
    "    Post_Learning(train_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kWzQv798OiQ-",
    "outputId": "57cabf2c-5c42-4084-e263-e00eaeb695ec",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "lightgbm training fold 1\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Number of positive: 18850, number of negative: 2303\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001695 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 13822\n",
      "[LightGBM] [Info] Number of data points in the train set: 21153, number of used features: 43\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.891127 -> initscore=2.102300\n",
      "[LightGBM] [Info] Start training from score 2.102300\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[24]\ttraining's auc: 0.897924\ttraining's f1score: 0.584616\tvalid_1's auc: 0.753874\tvalid_1's f1score: 0.544304\n",
      "--------------------------------------------------\n",
      "lightgbm training fold 2\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Number of positive: 18917, number of negative: 2237\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004928 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13986\n",
      "[LightGBM] [Info] Number of data points in the train set: 21154, number of used features: 43\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.894252 -> initscore=2.134925\n",
      "[LightGBM] [Info] Start training from score 2.134925\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[17]\ttraining's auc: 0.882418\ttraining's f1score: 0.494358\tvalid_1's auc: 0.766754\tvalid_1's f1score: 0.492252\n",
      "lightgbm our out of folds CV f1score is 0.5188994558145035\n",
      "--------------------------------------------------\n",
      "xgboost training fold 1\n",
      "[0]\ttrain-logloss:0.65982\ttrain-f1score:0.09818\teval-logloss:0.66014\teval-f1score:0.09563\n",
      "[25]\ttrain-logloss:0.33291\ttrain-f1score:0.67701\teval-logloss:0.34018\teval-f1score:0.65175\n",
      "[50]\ttrain-logloss:0.27178\ttrain-f1score:0.67807\teval-logloss:0.29038\teval-f1score:0.64808\n",
      "[75]\ttrain-logloss:0.24903\ttrain-f1score:0.68752\teval-logloss:0.28228\teval-f1score:0.64836\n",
      "[100]\ttrain-logloss:0.23553\ttrain-f1score:0.69878\teval-logloss:0.28106\teval-f1score:0.65009\n",
      "[125]\ttrain-logloss:0.22544\ttrain-f1score:0.71141\teval-logloss:0.28070\teval-f1score:0.65343\n",
      "[150]\ttrain-logloss:0.21787\ttrain-f1score:0.72472\teval-logloss:0.28084\teval-f1score:0.65484\n",
      "[175]\ttrain-logloss:0.21118\ttrain-f1score:0.73258\teval-logloss:0.28122\teval-f1score:0.65636\n",
      "[200]\ttrain-logloss:0.20460\ttrain-f1score:0.74316\teval-logloss:0.28169\teval-f1score:0.65741\n",
      "[212]\ttrain-logloss:0.20190\ttrain-f1score:0.74901\teval-logloss:0.28189\teval-f1score:0.65631\n",
      "--------------------------------------------------\n",
      "xgboost training fold 2\n",
      "[0]\ttrain-logloss:0.65967\ttrain-f1score:0.09563\teval-logloss:0.66016\teval-f1score:0.09818\n",
      "[25]\ttrain-logloss:0.33102\ttrain-f1score:0.65903\teval-logloss:0.34111\teval-f1score:0.65107\n",
      "[50]\ttrain-logloss:0.27069\ttrain-f1score:0.66417\teval-logloss:0.29197\teval-f1score:0.64849\n",
      "[75]\ttrain-logloss:0.24799\ttrain-f1score:0.67199\teval-logloss:0.28361\teval-f1score:0.65296\n",
      "[100]\ttrain-logloss:0.23563\ttrain-f1score:0.68586\teval-logloss:0.28200\teval-f1score:0.65558\n",
      "[125]\ttrain-logloss:0.22915\ttrain-f1score:0.69579\teval-logloss:0.28190\teval-f1score:0.65946\n",
      "[150]\ttrain-logloss:0.22199\ttrain-f1score:0.70614\teval-logloss:0.28214\teval-f1score:0.65888\n",
      "[175]\ttrain-logloss:0.21581\ttrain-f1score:0.71553\teval-logloss:0.28243\teval-f1score:0.65991\n",
      "[200]\ttrain-logloss:0.21024\ttrain-f1score:0.72735\teval-logloss:0.28290\teval-f1score:0.65961\n",
      "[208]\ttrain-logloss:0.20829\ttrain-f1score:0.73048\teval-logloss:0.28302\teval-f1score:0.65985\n",
      "xgboost our out of folds CV f1score is 0.6291361629633896\n",
      "Epoch 1/50\n",
      "1058/1058 [==============================] - 3s 2ms/step - loss: 0.3633 - accuracy: 0.8732 - val_loss: 0.2834 - val_accuracy: 0.9030 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "1058/1058 [==============================] - 2s 2ms/step - loss: 0.3037 - accuracy: 0.8969 - val_loss: 0.2818 - val_accuracy: 0.9025 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "1058/1058 [==============================] - 2s 2ms/step - loss: 0.2967 - accuracy: 0.8975 - val_loss: 0.2812 - val_accuracy: 0.9024 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "1058/1058 [==============================] - 2s 2ms/step - loss: 0.2954 - accuracy: 0.8988 - val_loss: 0.2814 - val_accuracy: 0.9023 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "1058/1058 [==============================] - 2s 2ms/step - loss: 0.2925 - accuracy: 0.8990 - val_loss: 0.2816 - val_accuracy: 0.9020 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "1058/1058 [==============================] - 2s 2ms/step - loss: 0.2907 - accuracy: 0.8988 - val_loss: 0.2820 - val_accuracy: 0.9024 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "1058/1058 [==============================] - 2s 2ms/step - loss: 0.2904 - accuracy: 0.8998 - val_loss: 0.2816 - val_accuracy: 0.9025 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "1058/1058 [==============================] - 2s 2ms/step - loss: 0.2906 - accuracy: 0.8991 - val_loss: 0.2823 - val_accuracy: 0.9026 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "1058/1058 [==============================] - 2s 2ms/step - loss: 0.2900 - accuracy: 0.8992 - val_loss: 0.2820 - val_accuracy: 0.9020 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "1058/1058 [==============================] - 2s 2ms/step - loss: 0.2897 - accuracy: 0.8993 - val_loss: 0.2829 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "1058/1058 [==============================] - 2s 2ms/step - loss: 0.2893 - accuracy: 0.8991 - val_loss: 0.2829 - val_accuracy: 0.9019 - lr: 9.0484e-04\n",
      "Epoch 12/50\n",
      "1058/1058 [==============================] - 2s 2ms/step - loss: 0.2882 - accuracy: 0.8993 - val_loss: 0.2826 - val_accuracy: 0.9023 - lr: 8.1873e-04\n",
      "Epoch 13/50\n",
      "1058/1058 [==============================] - 2s 2ms/step - loss: 0.2901 - accuracy: 0.8983 - val_loss: 0.2828 - val_accuracy: 0.9019 - lr: 7.4082e-04\n",
      "Epoch 14/50\n",
      "1058/1058 [==============================] - 2s 2ms/step - loss: 0.2896 - accuracy: 0.8980 - val_loss: 0.2814 - val_accuracy: 0.9024 - lr: 6.7032e-04\n",
      "Epoch 15/50\n",
      "1058/1058 [==============================] - 2s 2ms/step - loss: 0.2880 - accuracy: 0.8997 - val_loss: 0.2816 - val_accuracy: 0.9025 - lr: 6.0653e-04\n",
      "Epoch 16/50\n",
      "1058/1058 [==============================] - 2s 2ms/step - loss: 0.2872 - accuracy: 0.8991 - val_loss: 0.2818 - val_accuracy: 0.9023 - lr: 5.4881e-04\n",
      "Epoch 17/50\n",
      "1058/1058 [==============================] - 2s 2ms/step - loss: 0.2879 - accuracy: 0.8992 - val_loss: 0.2823 - val_accuracy: 0.9019 - lr: 4.9659e-04\n",
      "Epoch 18/50\n",
      "1058/1058 [==============================] - 2s 2ms/step - loss: 0.2877 - accuracy: 0.8990 - val_loss: 0.2822 - val_accuracy: 0.9020 - lr: 4.4933e-04\n",
      "Epoch 19/50\n",
      "1058/1058 [==============================] - 2s 2ms/step - loss: 0.2878 - accuracy: 0.9000 - val_loss: 0.2821 - val_accuracy: 0.9022 - lr: 4.0657e-04\n",
      "Epoch 20/50\n",
      "1058/1058 [==============================] - 2s 2ms/step - loss: 0.2872 - accuracy: 0.8994 - val_loss: 0.2815 - val_accuracy: 0.9024 - lr: 3.6788e-04\n",
      "Epoch 21/50\n",
      "1058/1058 [==============================] - 2s 2ms/step - loss: 0.2871 - accuracy: 0.9001 - val_loss: 0.2816 - val_accuracy: 0.9025 - lr: 3.3287e-04\n",
      "Epoch 22/50\n",
      "1058/1058 [==============================] - 2s 2ms/step - loss: 0.2869 - accuracy: 0.8998 - val_loss: 0.2817 - val_accuracy: 0.9024 - lr: 3.0119e-04\n",
      "Epoch 23/50\n",
      "1058/1058 [==============================] - 2s 2ms/step - loss: 0.2869 - accuracy: 0.8993 - val_loss: 0.2816 - val_accuracy: 0.9024 - lr: 2.7253e-04\n",
      "Epoch 24/50\n",
      "1058/1058 [==============================] - 2s 2ms/step - loss: 0.2863 - accuracy: 0.8990 - val_loss: 0.2816 - val_accuracy: 0.9019 - lr: 2.4660e-04\n",
      "Epoch 25/50\n",
      "1058/1058 [==============================] - 2s 2ms/step - loss: 0.2876 - accuracy: 0.8981 - val_loss: 0.2815 - val_accuracy: 0.9019 - lr: 2.2313e-04\n",
      "Epoch 26/50\n",
      "1058/1058 [==============================] - 2s 2ms/step - loss: 0.2879 - accuracy: 0.8992 - val_loss: 0.2816 - val_accuracy: 0.9019 - lr: 2.0190e-04\n",
      "Epoch 27/50\n",
      "1058/1058 [==============================] - 2s 2ms/step - loss: 0.2866 - accuracy: 0.9003 - val_loss: 0.2816 - val_accuracy: 0.9024 - lr: 1.8268e-04\n",
      "Epoch 28/50\n",
      "1058/1058 [==============================] - 2s 2ms/step - loss: 0.2873 - accuracy: 0.9003 - val_loss: 0.2818 - val_accuracy: 0.9023 - lr: 1.6530e-04\n",
      "Epoch 29/50\n",
      "1058/1058 [==============================] - 2s 2ms/step - loss: 0.2874 - accuracy: 0.8991 - val_loss: 0.2815 - val_accuracy: 0.9024 - lr: 1.4957e-04\n",
      "Epoch 30/50\n",
      "1058/1058 [==============================] - 2s 2ms/step - loss: 0.2869 - accuracy: 0.9000 - val_loss: 0.2815 - val_accuracy: 0.9020 - lr: 1.3534e-04\n",
      "Epoch 31/50\n",
      "1058/1058 [==============================] - 2s 2ms/step - loss: 0.2877 - accuracy: 0.8989 - val_loss: 0.2816 - val_accuracy: 0.9020 - lr: 1.2246e-04\n",
      "Epoch 32/50\n",
      "1058/1058 [==============================] - 2s 2ms/step - loss: 0.2864 - accuracy: 0.9000 - val_loss: 0.2814 - val_accuracy: 0.9023 - lr: 1.1080e-04\n",
      "Epoch 33/50\n",
      "1058/1058 [==============================] - 2s 2ms/step - loss: 0.2861 - accuracy: 0.9003 - val_loss: 0.2814 - val_accuracy: 0.9022 - lr: 1.0026e-04\n",
      "Epoch 34/50\n",
      "1058/1058 [==============================] - 2s 2ms/step - loss: 0.2859 - accuracy: 0.8997 - val_loss: 0.2813 - val_accuracy: 0.9024 - lr: 9.0718e-05\n",
      "Epoch 35/50\n",
      "1058/1058 [==============================] - 2s 2ms/step - loss: 0.2868 - accuracy: 0.9002 - val_loss: 0.2817 - val_accuracy: 0.9024 - lr: 8.2085e-05\n",
      "Epoch 36/50\n",
      "1058/1058 [==============================] - 2s 2ms/step - loss: 0.2861 - accuracy: 0.9001 - val_loss: 0.2815 - val_accuracy: 0.9024 - lr: 7.4274e-05\n",
      "Epoch 37/50\n",
      "1058/1058 [==============================] - 2s 2ms/step - loss: 0.2865 - accuracy: 0.8990 - val_loss: 0.2816 - val_accuracy: 0.9022 - lr: 6.7206e-05\n",
      "Epoch 38/50\n",
      "1058/1058 [==============================] - 2s 2ms/step - loss: 0.2873 - accuracy: 0.8995 - val_loss: 0.2814 - val_accuracy: 0.9023 - lr: 6.0810e-05\n",
      "Epoch 39/50\n",
      "1058/1058 [==============================] - 2s 2ms/step - loss: 0.2860 - accuracy: 0.9000 - val_loss: 0.2816 - val_accuracy: 0.9024 - lr: 5.5023e-05\n",
      "Epoch 40/50\n",
      "1058/1058 [==============================] - 2s 2ms/step - loss: 0.2862 - accuracy: 0.9002 - val_loss: 0.2814 - val_accuracy: 0.9022 - lr: 4.9787e-05\n",
      "Epoch 41/50\n",
      "1058/1058 [==============================] - 2s 2ms/step - loss: 0.2872 - accuracy: 0.8996 - val_loss: 0.2816 - val_accuracy: 0.9023 - lr: 4.5049e-05\n",
      "Epoch 42/50\n",
      "1058/1058 [==============================] - 2s 2ms/step - loss: 0.2868 - accuracy: 0.8997 - val_loss: 0.2814 - val_accuracy: 0.9023 - lr: 4.0762e-05\n",
      "Epoch 43/50\n",
      "1058/1058 [==============================] - 2s 2ms/step - loss: 0.2856 - accuracy: 0.9003 - val_loss: 0.2814 - val_accuracy: 0.9023 - lr: 3.6883e-05\n",
      "Epoch 44/50\n",
      "1058/1058 [==============================] - 2s 2ms/step - loss: 0.2884 - accuracy: 0.8992 - val_loss: 0.2815 - val_accuracy: 0.9024 - lr: 3.3373e-05\n",
      "Epoch 45/50\n",
      "1058/1058 [==============================] - 2s 2ms/step - loss: 0.2861 - accuracy: 0.9008 - val_loss: 0.2815 - val_accuracy: 0.9022 - lr: 3.0197e-05\n",
      "Epoch 46/50\n",
      "1058/1058 [==============================] - 2s 2ms/step - loss: 0.2862 - accuracy: 0.9007 - val_loss: 0.2816 - val_accuracy: 0.9020 - lr: 2.7324e-05\n",
      "Epoch 47/50\n",
      "1058/1058 [==============================] - 2s 2ms/step - loss: 0.2862 - accuracy: 0.9007 - val_loss: 0.2816 - val_accuracy: 0.9022 - lr: 2.4724e-05\n",
      "Epoch 48/50\n",
      "1058/1058 [==============================] - 2s 2ms/step - loss: 0.2871 - accuracy: 0.9002 - val_loss: 0.2817 - val_accuracy: 0.9020 - lr: 2.2371e-05\n",
      "Epoch 49/50\n",
      "1058/1058 [==============================] - 2s 2ms/step - loss: 0.2865 - accuracy: 0.9008 - val_loss: 0.2816 - val_accuracy: 0.9020 - lr: 2.0242e-05\n",
      "Epoch 50/50\n",
      "1058/1058 [==============================] - 2s 2ms/step - loss: 0.2870 - accuracy: 0.8995 - val_loss: 0.2814 - val_accuracy: 0.9020 - lr: 1.8316e-05\n"
     ]
    }
   ],
   "source": [
    "#å­¦ç¿’ã®å®Ÿè¡Œ\n",
    "Learning_and_Predicting(train_df, test_df, features, categorical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Postprocessing\n",
    "\n",
    "def Postprocessing(train_df, test_df):\n",
    "    #æœ€é©ãªé–¾å€¤ã‚’è¦‹ã¤ã‘ã‚‹é–¢æ•°\n",
    "    def find_best_threshold_and_score(y_true, y_pred_proba):\n",
    "        best_threshold = 0\n",
    "        best_score = 0\n",
    "        for threshold in np.linspace(0, 1, 1001):\n",
    "            score = f1_score(y_true, y_pred_proba >= threshold, average='macro')\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_threshold = threshold\n",
    "        return best_threshold, best_score\n",
    "    \n",
    "    # ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã™ã‚‹äºˆæ¸¬ç¢ºç‡\n",
    "    nn_model = load_model(f'nn_stacking_model_seed{CFG.seed}_ver{CFG.VER}.h5')\n",
    "    train_pred_proba_nn = nn_model.predict(train_df,oof_combined_features_scaled).flatten()\n",
    "    \n",
    "    # æœ€é©ãªé–¾å€¤ã¨ã‚¹ã‚³ã‚¢ã‚’æ±‚ã‚ã‚‹\n",
    "    best_threshold_nn, best_score_nn = find_best_threshold_and_score(train_df[CFG.target_col], train_pred_proba_nn)\n",
    "    print(f'NN Best Threshold: {best_threshold_nn}, Best F1 Score: {best_score_nn}')\n",
    "    \n",
    "    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«å¯¾ã™ã‚‹æœ€çµ‚äºˆæ¸¬\n",
    "    test_pred_proba_nn = nn_model.predict(test_combined_features_scaled).flatten()\n",
    "    test_final_predictions_nn = (test_pred_proba_nn >= best_threshold_nn).astype(int)\n",
    "    # æœ€çµ‚äºˆæ¸¬çµæœã‚’ã‚³ãƒ³ãƒšæå‡ºç”¨ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§CSVãƒ•ã‚¡ã‚¤ãƒ«ã«å‡ºåŠ›\n",
    "    submission_df_nn = pd.DataFrame({'Id': test_df.index, 'target': test_final_predictions_nn}).reset_index(drop=True)\n",
    "    submission_df_nn['Id'] = submission_df_nn.index + 4230\n",
    "    submission_df_nn.to_csv(f'stacking_nn_submission_best_score{best_score_nn:.4f}_seed{CFG.seed}_ver{CFG.VER}_{CFG.AUTHOR}.csv', header=False, index=False)\n",
    "    \n",
    "    \n",
    "    # ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã™ã‚‹äºˆæ¸¬ç¢ºç‡\n",
    "    lr_model = pickle.load(open(f'lr_stacking_model_seed{CFG.seed}_ver{CFG.VER}.pkl','rb'))\n",
    "    train_pred_proba_lr = lr_model.predict_proba(oof_combined_features_scaled)[:, 1]\n",
    "    \n",
    "    # æœ€é©ãªé–¾å€¤ã¨ã‚¹ã‚³ã‚¢ã‚’æ±‚ã‚ã‚‹\n",
    "    best_threshold_lr, best_score_lr = find_best_threshold_and_score(train_df[CFG.target_col], train_pred_proba_nn)\n",
    "    print(f'LR Best Threshold: {best_threshold_lr}, Best F1 Score: {best_score_lr}')\n",
    "    \n",
    "    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«å¯¾ã™ã‚‹æœ€çµ‚äºˆæ¸¬\n",
    "    test_pred_proba_lr = lr_model.predict_proba(test_combined_features_scaled)[:, 1]\n",
    "    test_final_predictions_lr = (test_pred_proba_lr >= best_threshold_lr).astype(int)\n",
    "    # æœ€çµ‚äºˆæ¸¬çµæœã‚’ã‚³ãƒ³ãƒšæå‡ºç”¨ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§CSVãƒ•ã‚¡ã‚¤ãƒ«ã«å‡ºåŠ›\n",
    "    submission_df_lr = pd.DataFrame({'Id': test_df.index, 'target': test_final_predictions_lr}).reset_index(drop=True)\n",
    "    submission_df_lr['Id'] = submission_df_lr.index + 4230\n",
    "    submission_df_lr.to_csv(f'stacking_lr_submission_best_score{best_score_lr:.4f}_seed{CFG.seed}_ver{CFG.VER}_{CFG.AUTHOR}.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Postprocessing() takes 0 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15284/3766098792.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#äºˆæ¸¬ã®å®Ÿè¡Œ\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mPostprocessing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcategorical_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: Postprocessing() takes 0 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "#äºˆæ¸¬ã®å®Ÿè¡Œ\n",
    "Postprocessing(train_df, test_df)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "13858cd0c554419bb867071b5c810b30": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4074a8c3b85548ed9424375a19416c7d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "573ab758539147f1a2bc21ac9f0347ce": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5de7c5cebd814e1caa8437775031ea12": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6762ef5aab7e4d18b98e7d837dedc03f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6f14799525cf44158bbffc0e9a08891b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b7dceec9910a46cea90915015ca58358",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_573ab758539147f1a2bc21ac9f0347ce",
      "value": "100%"
     }
    },
    "76ddf1599bb144b2a6b4573027c6f662": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4074a8c3b85548ed9424375a19416c7d",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_13858cd0c554419bb867071b5c810b30",
      "value": " 1000/1000 [00:19&lt;00:00, 37.91it/s]"
     }
    },
    "b0ab4a4e03b242e2ae3610b6a52b5c14": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b7dceec9910a46cea90915015ca58358": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d239d2ff297a4c00aa350f8699ca6bb1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5de7c5cebd814e1caa8437775031ea12",
      "max": 1000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b0ab4a4e03b242e2ae3610b6a52b5c14",
      "value": 1000
     }
    },
    "ef58b4abcf0f43cd90a523a3875a2f28": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6f14799525cf44158bbffc0e9a08891b",
       "IPY_MODEL_d239d2ff297a4c00aa350f8699ca6bb1",
       "IPY_MODEL_76ddf1599bb144b2a6b4573027c6f662"
      ],
      "layout": "IPY_MODEL_6762ef5aab7e4d18b98e7d837dedc03f"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
